---
title: "充分利用你的显存，智能调节学习率"
date: 2022-09-09T18:18:05+08:00
lastmod: 2022-09-09T09:19:06+08:00
draft: false
featured_image: "https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img_title.jpg"
description: "替新手踩坑，哈哈哈"
tags:
- Deep_learning
categories:
- 深度学习
- 图像分割
series:
- 《深度学习》学习笔记
comment : true
---

## 一些新手比较容易踩的坑
### batch_size 大小设置
先讲batch_size的功效：
1. 增大batch_size的大小，可以加快训练速度
2. 增大batch_size的大小，可以使得你使用的batch normalization更稳定，训练效果最好。

副作用：
1. batch_size越大，需要的显存就越大

> 相信所有人都经历过cuda out of memory报错，让人很心烦！那原因真的一定是batch_size过大吗？

* 第一种情况，是在验证的时候，没有加`with torch.no_grad()`，在验证的时候是不需要梯度反向传播的！
* 第二种情况是确实模型太大，超显存了。
* **相信小伙伴都遇到过第三种情况：你设batch_size为2还报out of memory，任务管理器里明明显示还有很多显存，那就需要注意你的num_worker数量了。** *如果你的num_worker比较大的话，cpu多线程读取图片的速度是快于你的GPU速度的，这时候会增加你的显存。这和你的GPU以及CPU的性能都是有关的，需要合理设置！*

### 学习率设置
* 学习率设置太大，容易让模型训练不动
* batch_size调大之后，学习率是需要相应调大的
* 调参的时候通常使用大的学习率先开始训练，如果模型收敛不了再调更小
* 训练的时候一般采用学习率衰减的方法防止过拟合，一般使用step步进或者是模拟余弦退火算法来控制学习率大小。