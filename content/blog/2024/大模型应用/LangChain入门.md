---
title: "LangChain:LLMåº”ç”¨æ¡†æ¶"
date: 2024-01-14T18:18:05+08:00
lastmod: 2024-01-15T09:19:06+08:00
draft: false
featured_image: "https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/LLM/LangChain_title.jpg"
description: "å­¦ä¹ å¤§æ¨¡å‹åº”ç”¨çš„æ•´ä½“æµç¨‹ï¼Œä»¥åŠåº”ç”¨æŠ€å·§ã€‚"
tags:
- Deep_learning
categories:
- å¤§æ¨¡å‹
series:
- ã€ŠLLMã€‹
comment : true
---

### LangChainï¼šLLMåº”ç”¨æ¡†æ¶

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•åœºæ™¯ï¼ˆLangChainç»“åˆæœ¬åœ°çŸ¥è¯†åº“å¯¹ç”¨æˆ·çš„æé—®è¿›è¡Œå›ç­”ï¼‰çš„æ•´ä½“æµç¨‹ï¼š

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/LLM/img29.jpg)



#### æœ¬åœ°çŸ¥è¯†åº“å¤„ç†

é¦–å…ˆéœ€è¦åŠ è½½æœ¬åœ°çŸ¥è¯†åº“æ–‡ä»¶ï¼Œå°†å…¶è½¬åŒ–ä¸ºçŸ¥è¯†å‘é‡åº“ã€‚

* æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼šå®ƒæœ¬èº«ç”¨äºå°†é«˜ç»´çš„ç¨€ç–æ•°æ®ï¼ˆæ–‡æœ¬ï¼‰è½¬åŒ–ä¸ºä½ç»´ç¨ å¯†æ•°æ®ã€‚ä¸»æµçš„åµŒå…¥æ¨¡å‹æœ‰**Word2Vec**ã€**GloVe**ã€**BERT**ã€**ERNIE**ã€**text2vec**ã€‚
* å‘é‡ç´¢å¼•åº“ï¼šå®ƒåœ¨ä¸åŒé¢†åŸŸä¸­ç”¨äºå­˜å‚¨å’Œé«˜æ•ˆæŸ¥è¯¢å‘é‡æ•°æ®ï¼Œä¸»è¦åº”ç”¨äºå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€è‡ªç„¶è¯­è¨€ç­‰é¢†åŸŸçš„ç›¸ä¼¼æ€§æœç´¢ä»»åŠ¡ã€‚ä¸»æµçš„åº“æœ‰**faiss**ã€**pgvector**ã€**Milvus**ã€**pinecone**ã€**weaviate**ã€**LanceDB**ã€**Chroma**ç­‰ã€‚

```python
from langchain.document_loaders import DirectoryLoader, unstructured
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma, faiss

import os

embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",
}

def load_documents(directory="books"):
    """
    Load documents
    """
    loader = DirectoryLoader(directory)
    documents = loader.load()
    # text: abcdefg, chunk_size:3, chunk_overlap: 1. resultï¼šabc cde efg
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)

    return split_docs


def load_embedding_model(model_name="ernie-tiny"):
    """
    Load embedding model
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}

    return HuggingFaceEmbeddings(
        model_name=embedding_model_dict[model_name], # è¿™é‡Œå¦‚æœä¸èƒ½åœ¨çº¿ä¸‹è½½ï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°ä¸‹è½½å¥½çš„æ–‡ä»¶
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()

    return db




if __name__ == '__main__':
    embeddings = load_embedding_model("text2vec3")
    if not os.path.exists("VectorStore"):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        db = Chroma(persist_directory="VectorStore", embedding_function=embeddings)
```

#### æœ¬åœ°å¤§æ¨¡å‹åŠ è½½

è¿™é‡Œæ˜¯ä½¿ç”¨ChatGLM-6Bä½œä¸ºæ¼”ç¤ºï¼Œä¸”æ²¡æœ‰ç»è¿‡å¾®è°ƒï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚æœ¬åœ°æ¨¡å‹æ¨ç†æœåŠ¡åœ¨ç«¯å£8000ä¸Šå¯åŠ¨ï¼Œæ¨¡å‹æ–‡ä»¶å¯ä»¥é€šè¿‡æœ¬åœ°æŒ‡å®šæ–‡ä»¶è¿›è¡ŒåŠ è½½ã€‚

#### ç”¨æˆ·è¾“å…¥å¤„ç†

* RetrievalQA æ˜¯ç”¨äºæ‰§è¡Œæ£€ç´¢å¼é—®ç­”ä»»åŠ¡çš„å‡½æ•°ã€‚æ£€ç´¢å¼é—®ç­”æ˜¯ä¸€ç§åŸºäºä¿¡æ¯æ£€ç´¢çš„é—®ç­”æ–¹æ³•ï¼Œå®ƒé€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚RetrievalQA å‡½æ•°çš„ä¸»è¦ä½œç”¨æ˜¯æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

  1. **é—®é¢˜ç†è§£ï¼š**RetrievalQA å‡½æ•°æ¥æ”¶ç”¨æˆ·æå‡ºçš„é—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿›è¡Œé—®é¢˜ç†è§£çš„å¤„ç†ã€‚è¿™åŒ…æ‹¬å¯¹é—®é¢˜è¿›è¡Œè¯­æ³•å’Œè¯­ä¹‰åˆ†æï¼Œä»¥ç¡®å®šé—®é¢˜çš„æ„å›¾ã€å…³é”®ä¿¡æ¯å’Œç›¸å…³ç‰¹å¾ã€‚

  2. **æ–‡æœ¬æ£€ç´¢ï¼š**ä¸€æ—¦é—®é¢˜è¢«ç†è§£ï¼ŒRetrievalQA å‡½æ•°ä½¿ç”¨é¢„å®šä¹‰çš„æ–‡æœ¬æ£€ç´¢æ–¹æ³•æ¥åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µæˆ–æ–‡æ¡£ã€‚è¿™å¯ä»¥ä½¿ç”¨å„ç§æŠ€æœ¯ï¼Œå¦‚å…³é”®è¯åŒ¹é…ã€å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æˆ–è€…åŸºäºè¯­ä¹‰çš„åŒ¹é…ç®—æ³•ã€‚

  3. **ç­”æ¡ˆç”Ÿæˆï¼š**åœ¨å®Œæˆæ–‡æœ¬æ£€ç´¢åï¼ŒRetrievalQA å‡½æ•°ä»æ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰‡æ®µä¸­æå–æˆ–ç”Ÿæˆå¯èƒ½çš„ç­”æ¡ˆã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°ç­”æ¡ˆæŠ½å–ã€æ–‡æœ¬æ‘˜è¦ã€å®ä½“è¯†åˆ«ç­‰æŠ€æœ¯ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„ç­”æ¡ˆã€‚

* gradioæ˜¯ç”¨æˆ·å¯¹è¯çš„ä¸€ä¸ªUIæ¡†æ¶ï¼Œç”¨äºå¯¹è¯å¼å¤§æ¨¡å‹é—®ç­”çš„ç•Œé¢ç”Ÿæˆã€‚

```python
from langchain.document_loaders import DirectoryLoader, unstructured
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma, faiss
from langchain.llms import ChatGLM
from langchain.chains import RetrievalQA

import gradio as gr

import os


embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",
}

def load_documents(directory="books"):
    """
    Load documents
    """
    loader = DirectoryLoader(directory)
    documents = loader.load()
    # text: abcdefg, chunk_size:3, chunk_overlap: 1. resultï¼šabc cde efg
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)

    return split_docs


def load_embedding_model(model_name="ernie-tiny"):
    """
    Load embedding model
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}

    return HuggingFaceEmbeddings(
        model_name=embedding_model_dict[model_name], # è¿™é‡Œå¦‚æœä¸èƒ½åœ¨çº¿ä¸‹è½½ï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°ä¸‹è½½å¥½çš„æ–‡ä»¶
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()

    return db




if __name__ == '__main__':
    embeddings = load_embedding_model("text2vec3")
    if not os.path.exists("VectorStore"):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        db = Chroma(persist_directory="VectorStore", embedding_function=embeddings)

    # éœ€è¦æ ¹æ®ChatGLMé¡¹ç›®æœ¬åœ°å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡åœ¨8000ç«¯å£
    llm = ChatGLM(
        endpoint="http://127.0.0.1:8000",
        max_token=80000,
        top_p=0.9
    )
    retriever = db.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        retriever=retriever
    )


    def chat(question, history):
        response = qa.run(question)
        return response

    demo = gr.ChatInterface(chat)
    demo.launch(inbrowser=True)
```

#### æ”¯æŒæ–‡æ¡£ä¸Šä¼ åŠŸèƒ½çš„å¯¹è¯é—®ç­”

é™¤äº†æå‰åœ¨ä»£ç åº“é‡Œæä¾›æ–‡æ¡£ï¼Œæˆ‘ä»¬åŒæ ·å¯ä»¥ä¸´æ—¶ä¸Šä¼ æ–‡æ¡£è¿›è¡ŒçŸ¥è¯†åº“çš„æ›´æ–°ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain.document_loaders import DirectoryLoader, unstructured
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma, faiss
from langchain.llms import ChatGLM
from langchain.chains import RetrievalQA

import gradio as gr

import time
import os


embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",
}

def load_documents(directory="books"):
    """
    Load documents
    """
    loader = DirectoryLoader(directory)
    documents = loader.load()
    # text: abcdefg, chunk_size:3, chunk_overlap: 1. resultï¼šabc cde efg
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)

    return split_docs


def load_embedding_model(model_name="ernie-tiny"):
    """
    Load embedding model
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}

    return HuggingFaceEmbeddings(
        model_name=embedding_model_dict[model_name], # è¿™é‡Œå¦‚æœä¸èƒ½åœ¨çº¿ä¸‹è½½ï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°ä¸‹è½½å¥½çš„æ–‡ä»¶
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()

    return db


# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video).
# Plus shows support for streaming text.


def print_like_dislike(x: gr.LikeData):
    print(x.index, x.value, x.liked)


def add_text(history, text):
    history = history + [(text, None)]
    return history, gr.Textbox(value="", interactive=False)


def add_file(history, file):
    # å–åˆ°æ–‡ä»¶å¤¹ï¼ˆä¸´æ—¶æ–‡ä»¶å¤¹ï¼‰
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    store_chroma(documents, embeddings)
    history = history + [((file.name,), None)]
    return history


def bot(history):
    message = history[-1][0]
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ"
    else:
        response = qa.run(message)
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history


if __name__ == "__main__":
    embeddings = load_embedding_model("text2vec3")
    if not os.path.exists("VectorStore"):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        db = Chroma(persist_directory="VectorStore", embedding_function=embeddings)

    # éœ€è¦æ ¹æ®ChatGLMé¡¹ç›®æœ¬åœ°å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡åœ¨8000ç«¯å£
    llm = ChatGLM(
        endpoint="http://127.0.0.1:8000",
        max_token=80000,
        top_p=0.9
    )
    retriever = db.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        retriever=retriever
    )
    
    with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
    )

    with gr.Row():
        txt = gr.Textbox(
            scale=4,
            show_label=False,
            placeholder="Enter text and press enter, or upload an image",
            container=False,
        )
        btn = gr.UploadButton("ğŸ“", file_types=["txt"])

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )

    chatbot.like(print_like_dislike, None, None)
    
		demo.queue()
    demo.launch()
```


