---
title: "åˆ†å‰²ç½‘ç»œæ¨¡å‹è½»é‡åŒ–æŠ€æœ¯"
date: 2023-03-20T18:18:05+08:00
lastmod: 2023-03-21T09:19:06+08:00
draft: false
featured_image: "https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img_title.jpg"
description: "åˆ†å‰²ç½‘ç»œçš„å‚æ•°é‡å¾€å¾€æ¯”åˆ†ç±»å’Œæ£€æµ‹ç½‘ç»œå¤§å¾—å¤šï¼Œä¸ºäº†è®©åˆ†å‰²ç½‘ç»œåœ¨å®é™…ä¸­åº”ç”¨ï¼Œéœ€è¦å¯¹å¤æ‚çš„ç½‘ç»œæ¨¡å‹è¿›è¡Œå‹ç¼©é‡åŒ–ã€‚"
tags:
- Deep_learning
categories:
- æ·±åº¦å­¦ä¹ 
series:
- ã€Šæ·±åº¦å­¦ä¹ ã€‹å­¦ä¹ ç¬”è®°
comment : true
---

## åˆ†å‰²æ¨¡å‹é‡åŒ–æŠ€æœ¯

**æœ¬æ–‡ä¸»è¦ä»‹ç»çš„å†…å®¹å¦‚ä¸‹ï¼š**
> * æé«˜ç½‘ç»œæ¨ç†æ•ˆç‡çš„åŸºæœ¬æŠ€æœ¯
> * è½»é‡åŒ–å®æ—¶åˆ†å‰²ç½‘ç»œå¸¸ç”¨çš„æ¶æ„
> * çŸ¥è¯†è’¸é¦

**æ–‡ä¸­æ¶‰åŠçš„éƒ¨åˆ†ä»£ç å…¨éƒ¨ä½¿ç”¨`Pytorchæ¡†æ¶`å®ç°ã€‚**

**æœ¬æ–‡çš„å¤§éƒ¨åˆ†å†…å®¹æ¥è‡ªäºç»¼è¿°æ–‡ç« **ï¼ˆ[On Efficient Real-Time Semantic Segmentation: A Survey](https://arxiv.org/pdf/2206.08605.pdf)ï¼‰

### æé«˜ç½‘ç»œæ¨ç†æ•ˆç‡çš„åŸºæœ¬æŠ€æœ¯
æœ¬èŠ‚å°†ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥ä»‹ç»ï¼š
> * é‡‡æ ·æŠ€æœ¯
> * é«˜æ•ˆå·ç§¯æŠ€æœ¯
> * æ®‹å·®è¿æ¥/è·³è¿‡è¿æ¥
> * è½»é‡åŒ–éª¨å¹²ç½‘ç»œ

#### é‡‡æ ·æŠ€æœ¯

é‡‡æ ·æŠ€æœ¯æ˜¯å‡å°‘æ¨ç†å»¶è¿Ÿæœ€å¸¸ç”¨çš„æ‰‹æ®µï¼Œé‡‡æ ·åˆ†ä¸ºä¸Šé‡‡æ ·å’Œä¸‹é‡‡æ ·ã€‚

ä¸‹é‡‡æ ·å¯ä»¥ç”¨æ¥é™ä½å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œåœ¨å¤§å‹ç½‘ç»œä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæ¥å¢åŠ æ·±å±‚å·ç§¯æ ¸çš„æ¥å—åœºã€‚é€šå¸¸åœ¨ç½‘ç»œæ—©æœŸå¯¹å›¾åƒè¿›è¡Œä¸‹é‡‡æ ·å¯ä»¥æ˜¾è‘—å‡å°‘ç½‘ç»œçš„æ¨ç†å»¶è¿Ÿï¼Œåœ¨æ·±å±‚ç½‘ç»œè¿›è¡Œä¸‹é‡‡æ ·ä¹Ÿå¯ä»¥æ›´å¥½åœ°æå–é«˜åˆ†è¾¨ç‡çš„ç»†èŠ‚ã€‚

å¸¸ç”¨çš„ä¸‹é‡‡æ ·æ–¹å¼æœ‰ä¸¤ç§ï¼Œä¸€æ˜¯ä½¿ç”¨`æœ€å¤§æ± åŒ–å±‚`ï¼ŒäºŒæ˜¯ä½¿ç”¨`æ­¥è¿›å·ç§¯`ï¼š
* æœ€å¤§æ± åŒ–å°†å›¾åƒåˆ†ä¸ºè‹¥å¹²ä¸ªæ± åŒ–å­åŒºåŸŸï¼Œåœ¨æ¯ä¸ªåŒºåŸŸä¸­å–æœ€å¤§çš„åƒç´ å€¼ã€‚
* æ­¥è¿›å·ç§¯åˆ™é€šè¿‡è°ƒæ•´æ­¥å¹…å¤§å°æ¥è°ƒæ•´å›¾ç‰‡çš„å¤§å°ï¼š
    æ ¹æ®è¾“å…¥å›¾åƒçš„å¤§å°$W\times W$ï¼Œå·ç§¯æ ¸çš„å¤§å°$F\times F$ï¼Œæ­¥é•¿$S$ï¼Œå¡«å……çš„æ•°é‡$P$æ¥è®¡ç®—è¾“å‡ºå›¾åƒçš„å¤§å°

$$W_{out} = \lvert\frac{W - F + 2P}{S}\rvert+1$$

```python
import torch.nn as nn

# æœ€å¤§æ± åŒ–å±‚(ä»¥ä¸‹é‡‡æ ·2å€ä¸ºä¾‹)
maxpooling = nn.MaxPool2d(kernel_size=2)
# æ­¥è¿›å·ç§¯ï¼ˆä»¥3*3å·ç§¯ä¸‹é‡‡æ ·2å€ä¸ºä¾‹ï¼‰
conv_downsample = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)
```

ä¸Šé‡‡æ ·çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†é‡å»ºè¾“å…¥åˆ†è¾¨ç‡çš„å›¾åƒï¼Œä¸Šé‡‡æ ·çš„æ–¹æ³•ä¸»è¦æœ‰ä¸‰ç§ï¼š
* æœ€è¿‘é‚»æ’å€¼
* åŒçº¿æ€§æ’å€¼
* è½¬ç½®å·ç§¯

ä»ä¸Šåˆ°ä¸‹è®¡ç®—ä»£ä»·è¶Šæ¥è¶Šè´µï¼Œé‡‡æ ·æ•ˆæœä¹Ÿè¶Šæ¥è¶Šå¥½ã€‚

```python
import torch
import torch.nn as nn

# æœ€è¿‘é‚»æ’å€¼ï¼ˆä¸Šé‡‡æ ·ä¸¤å€ï¼‰  
upsample1 = nn.Upsample(scale_factor=2, mode='nearest')
# åŒçº¿æ€§æ’å€¼
upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) # align_corners=Trueè¡¨ç¤ºä¿æŒè¾¹ç¼˜å¯¹é½
# è½¬ç½®å·ç§¯
conv_transpose = nn.ConvTranspose2d(in_channels=3, out_channels=3, 
kernel_size=4, stride=2, padding=1, bias=False)
```

#### é«˜æ•ˆå·ç§¯æŠ€æœ¯
é«˜æ•ˆå·ç§¯æŠ€æœ¯é€šå¸¸æ˜¯æ ‡å‡†å·ç§¯çš„å˜ä½“ï¼Œä½¿å¾—åœ¨ç›¸åŒå‚æ•°é‡çš„æƒ…å†µä¸‹è®¡ç®—é‡æ›´å°ï¼Œå·ç§¯ç½‘ç»œä¸­é€šå¸¸ä½¿ç”¨çš„é«˜æ•ˆå·ç§¯æœ‰5ç§ï¼š
* æ·±åº¦å¯åˆ†ç¦»å·ç§¯
* åˆ†ç»„å·ç§¯
* éå¯¹ç§°å·ç§¯
* ç“¶é¢ˆå—
* ç©ºæ´/æ‰©å¼ å·ç§¯

`æ·±åº¦å¯åˆ†ç¦»å·ç§¯`æ˜¯ç”±`æ·±åº¦å·ç§¯`å’Œ`é€ç‚¹å·ç§¯`ç»„åˆè€Œæˆï¼Œé€ç‚¹å·ç§¯ä¹Ÿå«$1\times 1$å·ç§¯ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡å‡†å·ç§¯åšä¸€ä¸ªå¯¹æ¯”æ¥ç†è§£ã€‚
> æ ‡å‡†$3\times 3$å·ç§¯

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img73.jpg)

> æ·±åº¦å¯åˆ†ç¦»å·ç§¯ = æ·±åº¦å·ç§¯ + $1\times 1$å·ç§¯

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img74.jpg)

åˆ†ç»„å·ç§¯çš„åŸç†æ˜¯å°†è¾“å…¥å’Œè¾“å‡ºé€šé“æ‹†åˆ†ä¸ºgç»„ï¼Œè¾“å‡ºæ»¤æ³¢å™¨ä»…åº”ç”¨äºå±äºç›¸åº”ç»„çš„è¾“å…¥é€šé“ï¼Œå‚æ•°é‡å’Œæ“ä½œéƒ½å‡å°‘äº†gå€ã€‚åˆ†ç»„å·ç§¯çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ç»„ä¸ç»„ä¹‹å‰ç¼ºé™·ä¿¡æ¯å…±äº«ï¼ŒCVPR 2018 paperï¼ˆ[ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)ï¼‰é€šè¿‡é€šé“æ´—ç‰Œæ“ä½œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

> åˆ†ç»„å·ç§¯

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img75.jpg)

`éå¯¹ç§°å·ç§¯`æˆ–å«`åˆ†è§£å·ç§¯`ï¼Œå°†$k\times k$å·ç§¯é‡æ„ä¸º$k\times 1$å’Œ$1\times k$å·ç§¯ã€‚

> éå¯¹ç§°å·ç§¯

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img76.jpg)

`ç“¶é¢ˆå—`æœ€åˆæ¥è‡ªäºCVPR 2016 paperï¼ˆ[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)ï¼‰ï¼Œåœ¨æ¨¡å—çš„è¾“å…¥ä½¿ç”¨$1\times 1$å·ç§¯æ¥å‡å°‘ç‰¹å¾æ˜ å°„é€šé“çš„æ•°é‡ï¼Œä¸­é—´ä½¿ç”¨å¤§å°ºå¯¸çš„å·ç§¯å’Œå¤§é‡çš„ç‰¹å¾é€šé“è¿›è¡Œè®¡ç®—ï¼Œåœ¨æ¨¡å—çš„è¾“å‡ºåˆä½¿ç”¨$1\times 1$å·ç§¯æ¥å‡å°‘ç‰¹é€šé“çš„æ•°é‡ã€‚

> ç“¶é¢ˆå—

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img77.jpg)

`æ‰©å¼ å·ç§¯`æœ€åˆæ¥è‡ªäºCVPR 2015 paperï¼ˆ[Modeling Local and Global Deformations in Deep Learning: Epitomic Convolution](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Papandreou_Modeling_Local_and_2015_CVPR_paper.pdf)ï¼‰ï¼Œå…¶é€šè¿‡åœ¨æ›´å¤§çš„è¾“å…¥çª—å£ä¸Šç¨€ç–åº”ç”¨æƒé‡å†…æ ¸ï¼Œåœ¨ä¸å¢åŠ å†…æ ¸å¤§å°çš„æƒ…å†µä¸‹å¯ç”¨æ›´å¤§çš„æ¥æ”¶åœºï¼Œå…¶ä½¿ç”¨è†¨èƒ€ç‡då†³å®šç¨€ç–åº”ç”¨çš„ç¨‹åº¦ã€‚

> æ‰©å¼ å·ç§¯

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img78.jpg)

ä¸‹è¡¨å±•ç¤ºäº†å®ƒä»¬çš„è®¡ç®—é‡ï¼š

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img79.jpg)

å®ç°ä»£ç å¦‚ä¸‹ï¼š
```python
import torch
import torch.nn as nn
import torch.nn.functional as F



class _DSConv(nn.Module):
    """
    Depthwise Separable Convolutions
    æ·±åº¦å¯åˆ†ç¦»å·ç§¯ = æ·±åº¦å·ç§¯ + 1*1ï¼ˆé€ç‚¹ï¼‰å·ç§¯
    """

    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):
        super(_DSConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),
            nn.BatchNorm2d(dw_channels),
            nn.ReLU(True),
            nn.Conv2d(dw_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True)
        )

    def forward(self, x):
        return self.conv(x)


class GroupConv2d(nn.Module):
    """
    Grouped Convolutions
    """
    def __init__(self, in_channels, out_channels, kernel_size, groups=1, stride=1, padding=1, bias=True):
        super(GroupConv2d, self).__init__()
        self.groups = groups
        self.convs = nn.ModuleList()
        for i in range(groups):
            self.convs.append(nn.Conv2d(in_channels//groups, out_channels//groups, kernel_size, stride=stride, padding=padding, bias=bias))
    
    def forward(self, x):
        # åˆ†ç»„
        x = torch.split(x, x.size(1)//self.groups, dim=1)
        # åˆ†åˆ«å·ç§¯
        x = [conv(item) for conv, item in zip(self.convs, x)]
        # åˆå¹¶è¾“å‡º
        x = torch.cat(x, dim=1)
        return x



class AsymmetricConv2d(nn.Module):
    """
    Asymmetric Convolutions
    """
    def __init__(self, in_channels, out_channels, kernel_size, padding=1):
        super(AsymmetricConv2d, self).__init__()

        # Define the asymmetric kernel
        self.padding = padding
        self.kernel_size = kernel_size
        self.left_kernel_size = (kernel_size, 1)
        self.right_kernel_size = (1, kernel_size)
        self.conv_left = nn.Conv2d(in_channels, out_channels, self.left_kernel_size, padding=(padding, padding//2))
        self.conv_right = nn.Conv2d(in_channels, out_channels, self.right_kernel_size, padding=(padding//2, padding))

    def forward(self, x):
        left = self.conv_left(x)
        right = self.conv_right(left)

        return right


class BottleneckBlock(nn.Module):
    """
    Bottleneck: contain BN and ReLU
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super(BottleneckBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels*2, kernel_size=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels*2)
        self.conv2 = nn.Conv2d(out_channels*2, out_channels*2, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels*2)
        self.conv3 = nn.Conv2d(out_channels*2, out_channels, kernel_size=1, stride=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        
    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        out = self.conv3(out)
        out = self.bn3(out)
        out = self.relu(out)
        
        return out

class DilatedConv(nn.Module):
    """
    Dilated Convolutions
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):
        super(DilatedConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation)

    def forward(self, x):
        out = self.conv(x)
        return out

```

#### æ®‹å·®è¿æ¥/è·³è¿‡è¿æ¥

å‰©ä½™è¿æ¥å’Œè·³è¿‡è¿æ¥å…è®¸ç½‘ç»œä¸­çš„æ•°æ®ç»•è¿‡æŸäº›æ“ä½œï¼Œæœ‰å¤šç§ç”¨é€”ã€‚ç¬¬ä¸€ç§å°±æ˜¯æ”¹å–„åå‘ä¼ æ’­æœŸé—´çš„æ¢¯åº¦æµï¼Œæœ€å…¸å‹çš„æ˜¯åœ¨CVPR 2016 paperï¼ˆ[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)ï¼‰æå‡ºçš„æ®‹å·®ç“¶é¢ˆå—ï¼Œä½¿ç½‘ç»œçš„ç»“æ„èƒ½å¤Ÿæ›´æ·±è€Œæé«˜æ€§èƒ½ï¼›ç¬¬äºŒç§å°±æ˜¯å¯¹åˆ†å‰²ç½‘ç»œæ—©æœŸç‰¹å¾çš„é‡ç”¨ï¼Œæœ€å…¸å‹çš„æ˜¯åœ¨MICCAI 2015 paperï¼ˆ[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf%EF%BC%89)ï¼‰æå‡ºçš„skip connectionï¼Œå°†æ—©æœŸæå–çš„ç‰¹å¾ç”¨äºä¸Šé‡‡æ ·é˜¶æ®µçš„å›¾åƒé‡å»ºã€‚

#### è½»é‡åŒ–éª¨å¹²ç½‘ç»œ
éšç€CNNçš„å‘å±•ï¼Œç½‘ç»œçš„æ·±åº¦è¶Šæ¥è¶Šå¤§ï¼Œå‚æ•°é‡è¶Šæ¥è¶Šå¤§ï¼Œä½†è¿™åœ¨å®é™…çš„åº”ç”¨ä¸­æ˜¯ä¸ç°å®çš„ã€‚éª¨å¹²ç½‘ç»œæ˜¯ç”±åˆ†ç±»ç½‘ç»œå»æ‰åˆ†ç±»å¤´å®ç°çš„ï¼Œä¸€èˆ¬ç”¨äºå¯¹å¤§å‹æ•°æ®çš„é¢„è®­ç»ƒï¼ˆImageNet-1kï¼‰ï¼Œæœ¬åœ°çš„åˆ†å‰²ç½‘ç»œåˆ™è½½å…¥é¢„è®­ç»ƒå¾—åˆ°çš„å‚æ•°ï¼Œç”¨äºåŠ é€Ÿç½‘ç»œçš„è®­ç»ƒå’Œè§£å†³åˆ†å‰²æ•°æ®é›†ä¸è¶³é—®é¢˜çš„ç“¶é¢ˆã€‚

å¸¸ç”¨çš„CNNè½»é‡åŒ–éª¨å¹²ç½‘ç»œæœ‰[`ResNetç³»åˆ—`](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)ï¼Œ[`Shufflenetç³»åˆ—`](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)ï¼Œ`Mobilenetç³»åˆ—`ï¼ˆ[V1](https://arxiv.org/pdf/1704.04861.pdf%EF%BC%89),[V2](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)ï¼‰ï¼Œ[`EfficientNetç³»åˆ—`](http://proceedings.mlr.press/v97/tan19a/tan19a.pdf)ã€‚

ä½†éšç€è¿‘å¹´æ¥transformerçš„å‡ºç°ï¼Œè¶…å¤§å‹ç½‘ç»œè¶…è¿‡äº†ä¼ ç»Ÿçš„CNNç½‘ç»œï¼ŒåŒæ ·çš„ä¹Ÿç”Ÿå‡ºäº†è¾ƒä¸ºè½»é‡çš„transformerè½»é‡åŒ–éª¨å¹²ç½‘ç»œã€‚
* CVPR 2022 paperï¼ˆ[MetaFormer is Actually What You Need for Vision](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.pdf)ï¼‰ï¼šæå‡ºäº†Poolformerçš„ç»“æ„ï¼Œåªä½¿ç”¨äº†æ± åŒ–å’Œé€šé“MLPï¼ŒæŠ›å¼ƒäº†å¤§å‹transformerç½‘ç»œä¸­çš„è‡ªæ³¨æ„åŠ›ç»“æ„ï¼Œä¸€å…±æä¾›äº†5ä¸ªä¸åŒå¤§å°çš„éª¨å¹²ç½‘ç»œï¼ˆ`S12`ï¼Œ`S24`ï¼Œ`S36`ï¼Œ`M36`ï¼Œ`M48`ï¼‰ï¼Œæ³¨æ„çš„æ˜¯Sç³»åˆ—åœ¨ä¸åŒstageè¾“å‡ºçš„é€šé“æ•°éƒ½ä¸º[64, 128, 320, 512]ï¼Œ Mç³»åˆ—åœ¨ä¸åŒstageè¾“å‡ºçš„é€šé“æ•°éƒ½ä¸º[96, 192, 384, 768]ï¼Œä¸åŒçš„æ¯ä¸ªstageçš„blockæ•°é‡ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img80.jpg)

* NeurIPS 2022 paperï¼ˆ[Efï¬cientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/pdf/2206.01191.pdf)ï¼‰ï¼šæå‡ºäº†EfficientFormerçš„ç»“æ„ï¼Œåœ¨ç½‘ç»œçš„æ—©æœŸï¼ˆå‰2.5ä¸ªstageï¼‰ä½¿ç”¨äº†æ± åŒ–å’Œé€ç‚¹å·ç§¯ï¼Œåœ¨ç½‘ç»œåæœŸï¼ˆå1.5ä¸ªstageï¼‰ä½¿ç”¨ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›Transformer Blockï¼Œä¸€å…±æä¾›äº†ä¸‰ä¸ªä¸åŒå¤§å°çš„éª¨å¹²ç½‘ç»œï¼ˆ`L1`ï¼Œ`L2`ï¼Œ`L3`ï¼‰ï¼Œè¿™é‡Œçš„ä¸‰ä¸ªç‰ˆæœ¬åœ¨ä¸åŒçš„stageçš„é€šé“æ•°å’Œblockæ•°é‡éƒ½ä¸åŒã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img81.jpg)
å®ç°äº†åœ¨å»¶è¿Ÿæ¯”è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œä¾æ—§å…·æœ‰è¾ƒé«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸æ£€æµ‹ä»»åŠ¡ï¼ˆCOCO2017ï¼‰å’Œåˆ†å‰²ä»»åŠ¡ï¼ˆADE20Kï¼‰ä¸Šä¹Ÿè¶…è¿‡äº†PoolFormerï¼
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img82.jpg)
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img83.jpg)

* arxiv preprint 2022å¹´åº•ï¼ˆä¼°è®¡2023å¹´è¢«æŸä¸ªé¡¶ä¼šå½•ç”¨ï¼‰ paperï¼ˆ[Rethinking Vision Transformers for MobileNet Size and Speed](https://arxiv.org/pdf/2212.08059.pdf)ï¼‰åŒå¹´ï¼Œè¯¥ä½œè€…åˆå¯¹è‡ªå·±çš„ç½‘ç»œè¿›è¡Œäº†æ”¹è¿›æå‡ºäº†EfficientFormerV2ï¼Œä»¥æ›´å°çš„è®¡ç®—ä»£ä»·è¾¾åˆ°äº†å…ˆå‰çš„åˆ†ç±»æ€§èƒ½ï¼Œæä¾›äº†4ä¸ªç‰ˆæœ¬ï¼ˆ`S0`ï¼Œ`S1`ï¼Œ`S2`ï¼Œ`L`ï¼‰ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img84.jpg)
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img85.jpg)

### è½»é‡åŒ–å®æ—¶åˆ†å‰²ç½‘ç»œ
è½»é‡åŒ–åˆ†å‰²ç½‘ç»œä¸»è¦æœ‰å››ç§æ¶æ„ï¼š
> * ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„
> * å¤šåˆ†æ”¯æ¶æ„
> * å…ƒå­¦ä¹ 
> * å¿«é€Ÿæ³¨æ„åŠ›æœºåˆ¶

#### ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„

* ECCV 2018 paperï¼ˆ[ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper.pdf)ï¼‰ï¼šESPNetæå‡ºäº†é«˜æ•ˆçš„é‡‘å­—å¡”æ¨¡å—ï¼Œä½¿ç”¨$1\times 1$å·ç§¯æ¥å‡å°‘è¾“å…¥ç»´åº¦ï¼Œç„¶åä½¿ç”¨å…·æœ‰ä¸åŒè†¨èƒ€ç‡çš„å¹¶è¡Œå·ç§¯æ¥å¢åŠ æ¥æ”¶åœºã€‚ä¸ºäº†é¿å…å› ä¸åŒè†¨èƒ€ç‡è€Œå¯¼è‡´çš„ç½‘æ ¼å·¥ä»¶ï¼Œè¾“å‡ºæŒ‰å±‚æ¬¡æ±‚å’Œï¼Œç»“æœä¸²è”ï¼Œæœ€åé€šè¿‡å‰©ä½™è¿æ¥æ·»åŠ åˆ°è¾“å…¥ä¸­ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img86.jpg)
ESPNetæ•´ä½“ç»“æ„å¦‚ä¸‹ï¼Œå…¶æ•´ä½“ç»“æ„ä¸UNetç›¸ä¼¼ã€‚åœ¨ç½‘ç»œçš„æ—©æœŸä½¿ç”¨æå°‘æ»¤æ³¢å™¨çš„å·ç§¯å±‚ä¸‹é‡‡æ ·ï¼Œå¹¶ä½¿ç”¨ç›¸åŒå°ºå¯¸çš„å›¾åƒè¿›è¡Œæ‹¼æ¥ï¼Œç„¶åé€šè¿‡skip connectionæ¥é‡å»ºåˆ†è¾¨ç‡ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img87.jpg)

* VCIP 2017 paperï¼ˆ[LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation](https://arxiv.org/pdf/1707.03718.pdf%5D)ï¼‰ï¼šLinkNetåŒæ ·ä½¿ç”¨æ ‡å‡†çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„ï¼Œä¸è¿‡å‚æ•°é‡ç›¸è¾ƒUNetå°‘äº†å¾ˆå¤šï¼Œä»¥ResNet-18ä½œä¸ºéª¨å¹²ç½‘ç»œã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img98.jpg)

* CVPR 2019 paperï¼ˆ[In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images](https://openaccess.thecvf.com/content_CVPR_2019/papers/Orsic_In_Defense_of_Pre-Trained_ImageNet_Architectures_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.pdf)ï¼‰ï¼šé¦–å…ˆæ”¹æ–‡ç« ç»“åˆUNetå’ŒPSPNetçš„ç»“æ„ï¼Œå†é€šè¿‡é€‰å–è¾ƒä¸ºè½»é‡çš„éª¨å¹²ç½‘ç»œä»¥åŠç®€åŒ–ä¸Šé‡‡æ ·çš„æ­¥éª¤ï¼Œç»„åˆå‡ºäº†ä¸€ä¸ªåŸºç¡€æ¨¡å‹SwiftNetRN-18ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img99.jpg)
ä¸ºäº†å¢åŠ æ¥æ”¶åœºï¼Œè¯¥ä½œè€…å–æ¶ˆäº†é‡‘å­—å¡”çš„æ¨¡å—å¹¶ä½¿ç”¨ç­åˆ†è¾¨ç‡çš„ç‰¹å¾æå–è¾¾åˆ°äº†æ¯”é‡‘å­—å¡”æ± åŒ–æ¨¡å—æ›´å¥½çš„æ•ˆæœã€‚å¹¶è¯æ˜äº†å°æ¨¡å‹åœ¨æ•°æ®é‡è¶³å¤Ÿçš„æƒ…å†µä¸‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¹Ÿèƒ½è¾¾åˆ°é¢„è®­ç»ƒçš„æ•ˆæœã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img100.jpg)

* arxiv 2019 paperï¼ˆ[Fast-SCNN: Fast Semantic Segmentation Network](https://arxiv.org/pdf/1902.04502.pdf)ï¼‰ï¼šFast-SCNNç½‘ç»œç»“æ„ç»“åˆäº†åŒåˆ†æ”¯ç½‘ç»œå’Œå¤šåˆ†è¾¨ç‡è¾“å…¥çš„ä¸¤ç§ç½‘ç»œçš„ç‰¹ç‚¹ï¼Œåœ¨ç‰¹å¾æå–çš„ä¸­é—´é˜¶æ®µåˆ†ä¸ºä¸¤ä¸ªåˆ†æ”¯ï¼Œè¿™æ ·åŒåˆ†æ”¯å…±äº«ç‰¹å¾æå–å‰æœŸçš„æƒé‡ã€‚ä¸€ä¸ªåˆ†æ”¯æå–å±€éƒ¨ä¿¡æ¯ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯æå–å…¨å±€ç‰¹å¾ï¼Œæœ€åç›´æ¥è¿›è¡ŒåŒçº¿æ€§æ’å€¼æ¢å¤è¾“å…¥åˆ†è¾¨ç‡ã€‚æ³¨æ„åœ¨ç‰¹å¾æå–å‰æœŸä½¿ç”¨æ™®é€šå·ç§¯ï¼Œåœ¨é€šé“æ•°è¾ƒå¤šæ—¶ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œç“¶é¢ˆå—æ¥å‡å°‘è®¡ç®—é‡ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img88.jpg)

#### å¤šåˆ†æ”¯æ¶æ„

* ECCV 2018 paperï¼ˆ[ICNet for Real-Time Semantic Segmentation on High-Resolution Images](https://openaccess.thecvf.com/content_ECCV_2018/papers/Hengshuang_Zhao_ICNet_for_Real-Time_ECCV_2018_paper.pdf)ï¼‰ï¼šICNeté¦–å…ˆé€šè¿‡ä½åˆ†è¾¨ç‡å›¾åƒé¦–å…ˆé€šè¿‡å®Œæ•´çš„è¯­ä¹‰æ„ŸçŸ¥ç½‘ç»œï¼Œä»¥è·å¾—ç²—ç•¥çš„é¢„æµ‹åœ°å›¾ã€‚ç„¶åæå‡ºäº†çº§è”ç‰¹å¾èåˆå•å…ƒå’Œçº§è”æ ‡ç­¾å¼•å¯¼ç­–ç•¥ï¼Œä»¥æ•´åˆä¸­é«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œä»è€Œé€æ­¥ç»†åŒ–ç²—ç³™çš„è¯­ä¹‰å›¾ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img89.jpg)
ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œåœ¨å›¾åƒè¾ƒå°æ—¶é‡‡ç”¨å¤šæ¬¡å·ç§¯å¹¶ä½¿ç”¨è¾ƒå¤§çš„æ·±åº¦ï¼Œå¤§å°ºå¯¸çš„å›¾åƒåˆ™åˆ™å°†å·ç§¯çš„stageå‡å°‘ï¼Œä½¿ç”¨ä½åˆ†è¾¨ç‡ç›‘ç£çš„ç»“æœæ”¾å…¥æ›´é«˜çš„åˆ†è¾¨ç‡è¿›è¡Œfusionã€‚

* arxiv 2018 paperï¼ˆ[Contextnet: Exploring context and detail for semantic segmentation in real-time](https://arxiv.org/pdf/1805.04554.pdf)ï¼‰ï¼šå…¶æå‡ºçš„ContextNetä»¥ä¸¤ä¸ªåˆ†æ”¯ç»“åˆä¸ºåŸºç¡€ï¼Œä¸€ä¸ªåˆ†æ”¯è¾“å…¥å…¨åˆ†è¾¨ç‡ï¼Œæå–ç©ºé—´ç‰¹å¾ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯ä¸ºå°åˆ†è¾¨ç‡çš„è¾“å…¥ï¼Œæå–å±€éƒ¨ç‰¹å¾ã€‚å…¶ä¸­ä½¿ç”¨çš„ç“¶é¢ˆå—å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯ç»“æ„ä¸Fast-SCNNç±»ä¼¼ï¼ŒFastâ€”SCNNä¹Ÿæ˜¯æ”¹è¿›è‡ªæ­¤ç½‘ç»œï¼Œå°†ä½åˆ†è¾¨ç‡çš„è¾“å…¥ç›´æ¥å˜ä¸ºç‰¹å¾æå–çš„ä½åˆ†è¾¨ç‡ç‰¹å¾ï¼Œè¿™æ ·æ—¢å¯ä»¥é‡ç”¨ç‰¹å¾æå–å‚æ•°ï¼Œåˆå¯ä»¥æ›´å¥½æå–è¯­ä¹‰ä¿¡æ¯ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img106.jpg)

#### â€œå…ƒå­¦ä¹ â€æŠ€æœ¯
â€œå…ƒå­¦ä¹ â€å¹¶ä¸æ˜¯ä¸€ä¸ªä¸“æœ‰åè¯ï¼Œè¿™é‡Œç”¨æ¥æ³›æŒ‡æ‰€å­¦å‡½æ•°ç›´æ¥å½±å“æ‰‹å¤´ä»»åŠ¡æ‰€ç”¨æ¶æ„çš„æŠ€æœ¯ã€‚å®æ—¶è¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„å¤§å¤šæ•°å…ƒå­¦ä¹ ç¤ºä¾‹éƒ½å±äºç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰çš„èŒƒç•´ï¼Œè¿™æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–è®¾è®¡ç¥ç»ç½‘ç»œæ¶æ„è¿‡ç¨‹çš„æ–¹æ³•ã€‚
* arxiv 2020 paperï¼ˆ[Fasterseg: Searching for faster real-time semantic segmentation](https://arxiv.org/pdf/1912.10917.pdf)ï¼‰ï¼šFasterSegä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç¥ç»æœç´¢æ¶æ„(NAS)ï¼Œåœ¨ä¿æŒä¸‹é‡‡æ ·8å€çš„ä¸»å¹²ä¸å˜çš„åŒæ—¶ï¼Œå¯¹ç½‘ç»œçš„å…¶ä½™ç»“æ„è¿›è¡Œè‡ªåŠ¨æœç´¢ï¼Œç”ŸæˆFastSegæ¨¡å‹ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img90.jpg)
é€šè¿‡æœ€åçš„ç½‘ç»œæœç´¢ï¼Œå½¢æˆäº†ä¸€ä¸ªå¤šåˆ†æ”¯ç»“åˆçš„ç½‘ç»œï¼Œå¹¶ä¸”åœ¨ç½‘ç»œçš„åæœŸä½¿ç”¨äº†å¤§é‡çš„ç¼©æ”¾å·ç§¯ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img91.jpg)

#### å¿«é€Ÿæ³¨æ„åŠ›æœºåˆ¶

* IEEE Robotics and Automation Lettersï¼ˆäºŒåŒºï¼‰ 2021 paperï¼ˆ[Real-time Semantic Segmentation with Fast Attention](https://ieeexplore.ieee.org/ielaam/7083369/9223766/9265219-aam.pdf)ï¼‰ï¼šFANetçš„ç½‘ç»œæ˜¯åŸºäºç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„çš„ï¼Œå…¶ç»“æ„ç›¸ä¼¼äºUNetï¼Œå®ƒåœ¨skip connectionä¹‹å‰åŠ å…¥äº†ä¸€ä¸ªå¿«é€Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå°‘é‡åœ°å¢åŠ è®¡ç®—é‡ã€‚å®ƒä½¿ç”¨äº†ResNet-18å’ŒResNet-34çš„éª¨å¹²ç½‘ç»œåˆ›ç«‹äº†ä¸¤ä¸ªç‰ˆæœ¬FANet-18å’ŒFANet-34ä¸¤ä¸ªç‰ˆæœ¬ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img92.jpg)

### çŸ¥è¯†è’¸é¦
**æœ¬èŠ‚å†…å®¹ä¸»è¦ä»‹ç»çŸ¥è¯†è’¸é¦çš„ç±»å‹ï¼ŒåŸç†ï¼Œæ–¹å¼ï¼Œæœ€åä¼šä»‹ç»ä¸€ä¸¤ç§å…·ä½“çš„è’¸é¦æ–¹æ³•ã€‚**

**æœ¬èŠ‚å†…å®¹å¤§éƒ¨åˆ†åŸºäºæ¥è‡ªäºç»¼è¿°æ–‡ç« **ï¼ˆ[Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525.pdf)ï¼‰

çŸ¥è¯†è’¸é¦æœ€é‡è¦çš„å°±æ˜¯çŸ¥è¯†ï¼Œè¿™é‡Œçš„çŸ¥è¯†å…¶å®å°±æ˜¯æ•°æ®çš„ç›®æ ‡åˆ†å¸ƒã€‚æ—©æœŸçš„çŸ¥è¯†è’¸é¦æŠ€æœ¯é‡Œï¼Œstudentå­¦ä¹ çš„æ˜¯teacherçš„logitsï¼ˆå¯¹äºåˆ†ç±»ä»»åŠ¡è€Œè¨€ï¼Œä¸€èˆ¬å°†è¿›softmaxä¹‹å‰çš„scoreså«åšlogitsï¼Œsoftmaxè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå«soft labelsï¼Œå½“ç„¶äº†ï¼Œåè€…ä¹Ÿæœ‰å«logitsçš„ï¼‰ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img101.jpg)
åæ¥ä¸ºäº†æ”¹å–„çŸ¥è¯†è’¸é¦çš„æ•ˆæœï¼Œå„è·¯å¤§ä½¬å¼€å§‹èŠ±å¼å¼€å‘å„ç§ç‰¹å¾ç”¨äºå­¦ä¹ ï¼Œæ•´ä½“å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼Œ`åŸºäºå“åº”çš„çŸ¥è¯†`ï¼ˆresponse-based knowledgeï¼‰, `åŸºäºç‰¹å¾çš„çŸ¥è¯†`ï¼ˆfeature-based knowledgeï¼‰å’Œ`åŸºäºå…³ç³»çš„è’¸é¦`ï¼ˆrelation-based knowledgeï¼‰ã€‚

#### åŸºäºå“åº”çš„çŸ¥è¯†
å“åº”ä¸€èˆ¬æ˜¯æŒ‡æ•™å¸ˆæ¨¡å‹çš„æœ€åä¸€å±‚çš„å“åº”ï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯è®©å­¦ç”Ÿæ¨¡ä»¿è€å¸ˆçš„é¢„æµ‹ã€‚åˆ†ç±»ä»»åŠ¡é‡Œæœ€å¸¸ç”¨çš„åŸºäºå“åº”çš„çŸ¥è¯†å°±æ˜¯è½¯æ ‡ç­¾ï¼Œå³softmaxè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä¸€èˆ¬ä½¿ç”¨`KLå‘æ•£`ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
$$p(z_i, T) = \frac{\text{exp}(z_i/T)}{\sum_{j}\text{exp}(z_j/T)}$$
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img102.jpg)

#### åŸºäºç‰¹å¾çš„çŸ¥è¯†
åŸºäºç‰¹å¾çš„çŸ¥è¯†å…¶å®å°±æ˜¯ä¸­é—´å±‚çš„è¾“å‡ºï¼Œè‡ªä»2015å¹´çš„Fitnetsä¹‹åï¼Œå‡ºç°äº†å¾ˆå¤šèŠ±å¼åˆ©ç”¨å„ç§ä¸­é—´å±‚ç‰¹å¾è¿›è¡Œè’¸é¦çš„ç ”ç©¶ã€‚è¿™é‡Œé‡ç‚¹æä¸€ä¸‹ä¸­é—´å±‚åŒ¹é…æ¶‰åŠåˆ°çš„ä¸€ä¸ªå±‚é—´åŒ¹é…é—®é¢˜ï¼Œå¯¹äºç¦»çº¿è’¸é¦è€Œè¨€ï¼Œstudentå¯èƒ½å±‚æ•°ä¼šå°ä¸€äº›ï¼Œå¦‚ä½•å°†teacherçš„å±‚ä¸studentçš„å±‚è¿›è¡Œå¯¹åº”æˆ–åŒ¹é…å°±æ˜¯ä¸ªå…³é”®é—®é¢˜ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå±‚é—´åŒ¹é…åªèƒ½é ç»éªŒæˆ–è€…å®éªŒã€‚å¦å¤–ï¼Œä¸­é—´å±‚çš„ç‰¹å¾ç»´åº¦å¯èƒ½ä¼šä¸åŒ¹é…ï¼Œéœ€è¦æ ¹æ®æƒ…å†µè¿›è¡ŒæŠ•å½±æˆ–è€…è¯´å˜æ¢ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img103.jpg)

#### åŸºäºå…³ç³»çš„çŸ¥è¯†
è¿™ç§çŸ¥è¯†ä¸€èˆ¬æ˜¯æŒ‡çš„æ˜¯å›¾å½¢çŸ¥è¯†ï¼Œå®ƒè¡¨ç¤ºåœ¨ä»»æ„ä¸¤ä¸ªç‰¹å¾å›¾ä¹‹é—´çš„æ•°æ®å†…å…³ç³»ï¼Œé€šå¸¸ä½¿ç”¨ç‰¹å¾å›¾ä¹‹é—´çš„ç›¸ä¼¼åº¦è®¡ç®—æ¥è¡¡é‡ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img104.jpg)

#### è’¸é¦æ–¹æ³•
çŸ¥è¯†è’¸é¦çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºç¦»çº¿è’¸é¦ï¼Œåœ¨çº¿è’¸é¦å’Œè‡ªè’¸é¦ä¸‰ç§ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img105.jpg)
* ç¦»çº¿è’¸é¦ï¼šä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œç¦»çº¿è’¸é¦çš„ç¬¬ä¸€æ­¥å°±æ˜¯é€‰å–ä¸€ä¸ªæ€§èƒ½è¡¨ç°è¾ƒå¥½çš„æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šé¢„è®­ç»ƒã€‚ç¬¬äºŒæ­¥å°†è®­ç»ƒå¥½ä¹‹åçš„æƒå€¼åœ¨å­¦ç”Ÿè®­ç»ƒé˜¶æ®µè½½å…¥ï¼Œå¹¶é”å®šå‚æ•°ï¼Œåªåœ¨ç‰¹å¾è¾“å‡ºå’Œç»“æœè¾“å‡ºä¸Šè®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ å…¶æ•°æ®åˆ†å¸ƒï¼Œå­¦ç”Ÿæ¨¡å‹å‡ºäº†å­¦ä¹ æ ‡ç­¾æä¾›çš„æ•°æ®åˆ†å¸ƒè¿˜è¦å­¦ä¹ è€å¸ˆæ¨¡å‹å¤„ç†æ•°æ®çš„æ–¹æ³•ã€‚éœ€è¦æ³¨æ„çš„æ˜¯åœ¨å­¦ç”Ÿæ¨¡å‹è®­ç»ƒçš„æ—¶å€™ï¼Œæ•™å¸ˆæ¨¡å‹å¿…é¡»ä½¿ç”¨éªŒè¯æ¨¡å¼å¹¶é”å®šç‰¹å¾è¾“å‡ºéƒ¨åˆ†çš„æƒå€¼ï¼Œé˜²æ­¢è’¸é¦æŸå¤±å‡½æ•°å¯¹æ•™å¸ˆæ¨¡å‹çš„åå‘ä¼ æ’­ã€‚**å¦‚æœå­¦ç”Ÿæ¨¡å‹æ—¢è¦ä½¿ç”¨éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒï¼Œåˆè¦ä½¿ç”¨çŸ¥è¯†è’¸é¦ï¼ˆç‰¹åˆ«æ˜¯ç‰¹å¾è’¸é¦ï¼‰ï¼Œéœ€è¦å…ˆé”ä½é¢„è®­ç»ƒæƒé‡é”ä½è®­ç»ƒä¸€ä¼šå„¿å†è§£å¼€ï¼Œé˜²æ­¢æ—©æœŸè¿‡å¤šæŸå¤±çš„åå‘ä¼ æ’­ç ´åäº†é¢„è®­ç»ƒçš„æ•ˆæœã€‚**ç”±äºç¦»çº¿è’¸é¦æ¯”è¾ƒå®¹æ˜“å®ç°ï¼Œå¤§å¤šæ•°è’¸é¦éƒ½é‡‡ç”¨è¿™ç§æ–¹å¼ã€‚

* åœ¨çº¿è’¸é¦ï¼šé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯å­¦ç”Ÿå’Œè€å¸ˆåŒæ—¶å¼€å§‹ä»å¤´è®­ç»ƒï¼Œä¹Ÿå¯ä»¥æœ‰é¢å¤–éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒçš„å¸®åŠ©ã€‚ä½†æ˜¯è¿™ç§è®­ç»ƒæ–¹å¼ä¼šå¸¦æ¥å¾ˆå¤šçš„é—®é¢˜ï¼Œæ¯”å¦‚æ¨¡å‹å¤§å°ä¸ä¸€ï¼Œå­¦ç”Ÿå’Œè€å¸ˆçš„è®­ç»ƒé€Ÿåº¦ä¸åŒã€‚åˆæ¯”å¦‚å­¦ç”Ÿå’Œè€å¸ˆçš„æ¨¡å‹åŒæ—¶è½½å…¥ï¼Œè¿˜éœ€è¦è®¡ç®—ç‰¹å¾å›¾ä¹‹é—´çš„å…³ç³»æŸå¤±ï¼Œè¿™ä¼šä½¿GPUçš„å†…å­˜è´Ÿæ‹…å˜å¾—éå¸¸å¤§ã€‚è¿˜æœ‰å¯èƒ½å­˜åœ¨çš„é—®é¢˜æ˜¯å­¦ç”Ÿç½‘ç»œå’Œæ•™å¸ˆç½‘ç»œç»“æ„å·®è·è¾ƒå¤§ï¼Œå­¦ç”Ÿå­¦ä¹ äº†æ•™å¸ˆæ—©æœŸçš„çŸ¥è¯†ï¼Œå¹¶é™·å…¥äº†ä¸€ä¸ªå±€éƒ¨åˆ†å¸ƒå˜å¾—éš¾ä»¥è®­ç»ƒã€‚ç”±äºè¿™ç§æ–¹å¼è®­ç»ƒéš¾åº¦è¾ƒå¤§ï¼Œæ‰€ä»¥ç›¸å…³çš„å·¥ä½œæ¯”è¾ƒå°‘ã€‚

* è‡ªè’¸é¦ï¼šè‡ªè’¸é¦å°±æ˜¯è‡ªå·±åˆæ˜¯è€å¸ˆåˆæ˜¯å­¦ç”Ÿï¼Œè¿™å°±æ¯”è¾ƒåŠ±å¿—äº†ğŸ˜„ã€‚å¸¸è§çš„æ–¹å¼å°±æ˜¯ç½‘ç»œçš„ä½å±‚å­¦ä¹ é«˜å±‚ï¼ŒåæœŸå­¦ä¹ å‰æœŸç­‰ã€‚ç›¸å½“äºä¸€ä¸ªäººç»å†äº†è®¸å¤šï¼Œæ”¹æ‰äº†ä»¥å‰è‡ªå·±çš„åæ¯›ç—…äº¦æˆ–æ˜¯ä¸€ä¸ªäººåœ¨åæœŸå‘ç°äº†æ—©æœŸç»å†äº†å´æ²¡æœ‰æ‚Ÿå‡ºçš„é“ç†ï¼ï¼ˆè‡ªå·±çæ‰¯ï¼ŒğŸ¶ä¿å‘½ï¼‰

#### çŸ¥è¯†è’¸é¦è®ºæ–‡è§£è¯»

* CVPR 2019 paperï¼ˆ[Knowledge Adaptation for Efficient Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.pdf)ï¼‰ï¼šè¯¥æ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†é€‚åº”çš„æ–¹æ³•ï¼Œæ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œæ˜¯ç‹¬ç«‹çš„ï¼Œè’¸é¦çš„ä½ç½®åœ¨ä¸¤ä¸ªç½‘ç»œçš„è¾“å‡ºä½ç½®ã€‚ç¬¬ä¸€æ­¥ï¼Œæ•™å¸ˆç½‘ç»œé€šè¿‡è‡ªåŠ¨ç¼–ç å™¨å°†çŸ¥è¯†å‹ç¼©ä¸ºç´§å‡‘çš„æ ¼å¼æ¥è®©å­¦ç”Ÿå­¦ä¹ ã€‚ç¬¬äºŒæ­¥ï¼Œå­¦ç”Ÿç½‘ç»œé€šè¿‡`Feature Adapter`æ¥æ•è·æ•™å¸ˆç½‘ç»œçš„è¿œç¨‹ä¾èµ–å…³ç³»ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img93.jpg)

* CVPR 2019 paperï¼ˆ[A Comprehensive Overhaul of Feature Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.pdf)ï¼‰ï¼šè¯¥æ–‡ä½¿ç”¨çš„æ˜¯çŸ¥è¯†è’¸é¦ä¸­çš„ç‰¹å¾è’¸é¦æ–¹æ³•ã€‚é¦–å…ˆä»‹ç»äº†ç‰¹å¾è’¸é¦çš„ä¸€èˆ¬èŒƒå¼ï¼šæ ¹æ®ä¸åŒçš„ç‰¹å¾è’¸é¦æ–¹æ³•ï¼Œæ”¹å˜æ•™å¸ˆå˜æ¢Tï¼Œå­¦ç”Ÿå˜æ¢Sï¼Œæ•™å¸ˆä¸å­¦ç”Ÿçš„è·ç¦»dã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img94.jpg)
è¯¥æ–‡è®¾è®¡äº†ä¸å…¶ä»–è®ºæ–‡çš„ä¸åŒçš„æ•™å¸ˆå˜æ¢ï¼Œå­¦ç”Ÿå˜æ¢ï¼Œè·ç¦»å‡½æ•°ï¼Œä»¥åŠæ”¹å˜äº†è’¸é¦ç‰¹å¾çš„ä½ç½®ã€‚
    * æ•™å¸ˆå˜æ¢ï¼šMargin ReLU
    $$\sigma_m(x) = \text{max}(x, m)$$
    ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img95.jpg)

    * å­¦ç”Ÿå˜æ¢ï¼š$1\times 1$å·ç§¯
    * è’¸é¦çš„æŸå¤±å‡½æ•°ï¼šå°†è€å¸ˆä¸å­¦ç”Ÿçš„è·ç¦»ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¹¶å°†`partial L2`è·ç¦»å‡½æ•°å¯¹æ•™å¸ˆè½¬æ¢å’Œå­¦ç”Ÿè½¬æ¢åšè’¸é¦ã€‚
    $$L_{distill}=d_p(\sigma_{m_c}(F_t),r(F_s))$$
    $$d_p(T,S)=\sum_i^{W HC}\left\{\begin{matrix}0,\quad \text{if}\quad S_i\leqslant T_i \leqslant 0\\(T_i-S_i)^2\quad \text{otherwise}\end{matrix}\right.$$
    * ç‰¹å¾è’¸é¦çš„å…·ä½“ä½ç½®ï¼šè¯¥æ–‡è®¤ä¸ºå¦‚æœåœ¨å·ç§¯åé¢è·Ÿç€çš„BNå’ŒReLUä¹‹ååšç‰¹å¾è’¸é¦ï¼Œä¼šæŸå¤±éƒ¨åˆ†ä»è€å¸ˆçš„ç‰¹å¾åˆ†å¸ƒå¸¦è¿‡æ¥çš„ä¿¡æ¯ã€‚æ‰€ä»¥ä»–ä»¬å°†è¿›è¡Œè’¸é¦çš„ä½ç½®æ”¹å˜åˆ°äº†æ¯ä¸ªstageçš„æœ«å°¾å·ç§¯åå’ŒReLUæ¿€æ´»å‡½æ•°å‰ã€‚å› ä¸ºä½¿ç”¨ç¦»çº¿è’¸é¦ï¼Œæ•™å¸ˆæ¨¡å‹çš„å‚æ•°æ˜¯æœ€ä¼˜ç›®æ ‡åˆ†å¸ƒï¼Œå‚æ•°æ˜¯ä¸åŠ¨çš„ï¼Œä¸”è½½å…¥GPUæ—¶ä½¿ç”¨éªŒè¯æ¨¡å¼ï¼Œæ‰€ä»¥Batch Normalizationå¯ä»¥å¿½ç•¥ã€‚
    
    ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img96.jpg)

* CVPR 2019 paperï¼ˆ[Structured Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf)ï¼‰ï¼šæœ¬æ–‡ä¸“é—¨é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ç½‘ç»œæå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œç»“åˆç‰¹å¾è’¸é¦ï¼Œè¾“å‡ºåƒç´ è’¸é¦ï¼Œå¯¹æŠ—æ€§å­¦ä¹ è’¸é¦ç»„æˆäº†ä¸€ä¸ªç»“æ„åŒ–è’¸é¦çš„æ–¹æ³•ã€‚
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img97.jpg)

    * åŸºäºå…³ç³»çš„ç‰¹å¾è’¸é¦ï¼šè¯¥æ–‡ç« æå‡ºäº†ä¸€ç§è®¡ç®—ç‰¹å¾å›¾ç›¸ä¼¼åº¦çš„`Pair-wise loss`ï¼Œé¦–å…ˆé€šè¿‡æ± åŒ–æ¥æ§åˆ¶ç‰¹å¾å›¾è¾“å‡ºçš„å¤§å°ï¼Œç„¶åè®¡ç®—ç‰¹å¾å›¾ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥è®¡ç®—æŸå¤±ã€‚
    $$l_{pa}(S)=\frac{1}{(W^{'}\times H^{'})^2}\sum_{i\varepsilon R}\sum_{j\varepsilon R}(a_{ij}^s-a_{ij}^t)^2$$
    å…¶ä¸­$a_{ij}$ä»£è¡¨ä¸¤ä¸ªåƒç´ ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œ
    $$a_{ij}=f_i^{\top}f_j/(\|f_i \|_2\|f_j \|_2 )$$
    å®ç°ä»£ç å¦‚ä¸‹ï¼š
    ```python
    def similarity(feat):
    feat = feat.float()
    tmp = L2(feat).detach()
    feat = feat/tmp
    feat = feat.reshape(feat.shape[0],feat.shape[1],-1)
    return torch.einsum('icm,icn->imn', [feat, feat])


    def sim_dis_compute(f_S, f_T):
        sim_err = ((similarity(f_T) - similarity(f_S))**2)/((f_T.shape[-1]*f_T.shape[-2])**2)/f_T.shape[0]
        sim_dis = sim_err.sum()
        return sim_dis

    # pairwise feature loss
    class CriterionPairWiseforWholeFeatAfterPool(nn.Module):
        def __init__(self, scale, feat_ind):
            """
            inter pair-wise loss from inter feature maps
            scale ä»£è¡¨æœ€å¤§æ± åŒ–å±‚çš„å­åŒºåŸŸå åŸç‰¹å¾å›¾å¤§å°çš„æ¯”ä¾‹ã€‚
            æ¯”å¦‚2*2çš„å­åŒºåŸŸï¼Œç‰¹è¯Šå›¾å¤§å°ä¸º56*56ï¼Œscaleå°±åº”è¯¥ç­‰äº28/56ã€‚
            """
            super(CriterionPairWiseforWholeFeatAfterPool, self).__init__()
            self.criterion = sim_dis_compute
            self.feat_ind = feat_ind
            self.scale = scale

        def forward(self, preds_S, preds_T):
            feat_S = preds_S[self.feat_ind]
            feat_T = preds_T[self.feat_ind]
            feat_T.detach()

            total_w, total_h = feat_T.shape[2], feat_T.shape[3]
            patch_w, patch_h = int(total_w*self.scale), int(total_h*self.scale)
            maxpool = nn.MaxPool2d(kernel_size=(patch_w, patch_h), stride=(patch_w, patch_h), padding=0, ceil_mode=True) # change
            loss = self.criterion(maxpool(feat_S), maxpool(feat_T))
            return loss
    ```
    * è¾“å‡ºåƒç´ è’¸é¦ï¼š`Pixel-wise Loss`å€Ÿé‰´äº†å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨çš„è½¯æ ‡ç­¾çš„æ–¹æ³•ï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡å‹äº§ç”Ÿçš„ç±»æ”¹æˆæ¥ä½œä¸ºè®­ç»ƒå­¦ç”Ÿæ¨¡å‹çš„è½¯ç›®æ ‡ã€‚
    $$l_{pi}(S) = \frac{1}{W^{'}\times H^{'}}\sum_{i\varepsilon R}\text{KL}(q_i^s\|q_i^t)$$
    å…¶ä¸­$\text{KL}$ä»£è¡¨ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„Kullback-Leiblerå‘æ•£ã€‚ä»£ç å¦‚ä¸‹ï¼š
    ```python
    # Pixel loss
    class CriterionPixelWise(nn.Module):
        """
        reduceå‚æ•°æ˜¯è®¡ç®—CE lossæ¯ä¸ªmini batchçš„å’Œå¹³å‡
        """
        def __init__(self, ignore_index=255, use_weight=True, reduce=True):
            super(CriterionPixelWise, self).__init__()
            self.ignore_index = ignore_index
            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index, reduce=reduce)
            if not reduce:
                print("disabled the reduce.")

        def forward(self, preds_S, preds_T):
            preds_T.detach()
            assert preds_S.shape == preds_T.shape,'the output dim of teacher and student differ'
            N,C,W,H = preds_S.shape
            softmax_pred_T = F.softmax(preds_T.permute(0,2,3,1).contiguous().view(-1,C), dim=1)
            logsoftmax = nn.LogSoftmax(dim=1)
            loss = (torch.sum( - softmax_pred_T * logsoftmax(preds_S.permute(0,2,3,1).contiguous().view(-1,C))))/W/H
            return loss
    ```
    * å¯¹æŠ—æ€§å­¦ä¹ è’¸é¦ï¼š`Holistic distillation`é‡‡ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—å­¦ä¹ æ¥åˆ¶å®šæ•´ä½“è’¸é¦é—®é¢˜ã€‚ç´§å‡‘å‹ç½‘è¢«è§†ä¸ºä»¥è¾“å…¥RGBå›¾åƒIä¸ºæ¡ä»¶çš„ç”Ÿæˆå™¨ï¼Œè€Œé¢„æµ‹çš„åˆ†å‰²å›¾$Q^s$è¢«è§†ä¸ºå‡æ ·æœ¬ã€‚æˆ‘ä»¬é¢„è®¡$Q^s$ä¸$Q^t$ç›¸ä¼¼ï¼Œ$Q^t$æ˜¯æ•™å¸ˆé¢„æµ‹çš„åˆ†å‰²å›¾ï¼Œå¹¶å°½å¯èƒ½è¢«è§†ä¸ºçœŸå®æ ·æœ¬ã€‚`Wasserstein distance`ç”¨äºè¯„ä¼°çœŸå®åˆ†å¸ƒå’Œå‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå†™æˆå¦‚ä¸‹:
    $$l_{ho}(S,D) = E_{Q^s\sim p_s(Q^s)}[D(Q^s|I)] - E_{Q^t\sim p_t(Q^t)}[D(Q^t|I)]$$
    å…¶ä¸­$E$æ˜¯æœŸæœ›è¿ç®—ç¬¦ï¼Œ$D$æ˜¯åµŒå…¥ç½‘ç»œï¼Œä½œä¸ºGANä¸­çš„é‰´åˆ«å™¨ï¼Œå°†Qå’ŒIä¸€èµ·æŠ•å½±æˆæ•´ä½“åµŒå…¥åˆ†æ•°ã€‚æ¢¯åº¦æƒ©ç½šæ»¡è¶³äº†åˆ©æ™®å¸ŒèŒ¨çš„è¦æ±‚ã€‚

æ‰€ä»¥**ç»“æ„åŒ–è’¸é¦çš„æŸå¤±å‡½æ•°**ä¸ºï¼š
$$l(S,D) = l_{mc}(S) + \lambda_1(l_{pi}(S) + l_{pa}(S)) - \lambda_2l_{ho}(S,D)$$
å…¶ä¸­$pi$ä»£è¡¨è¾“å‡ºåƒç´ æŸå¤±ï¼Œ$pa$ä»£è¡¨æˆå¯¹ç‰¹å¾ç›¸ä¼¼åº¦æŸå¤±ï¼Œ$ho$ä»£è¡¨å¯¹æŠ—æ€§å­¦ä¹ è’¸é¦æŸå¤±ã€‚æ€»ä½“çš„è®­ç»ƒè¢«åˆ†ä¸ºä¸¤æ­¥ï¼š
> 1.è®­ç»ƒGANé‰´åˆ«å™¨ï¼Œè®­ç»ƒé‰´åˆ«å™¨ç­‰åŒäºæœ€å°åŒ–$ho$æŸå¤±ï¼Œä¸ºå­¦ç”Ÿç½‘ç»œçš„å‡æ ·æœ¬æä¾›ä½åµŒå…¥åˆ†æ•°ã€‚

> 2.è®­ç»ƒç´§å‡‘åˆ†å‰²ç½‘ç»œï¼Œæœ€å¤§é™åº¦å‡å°‘ä¸ä¸ç´§å‡‘åˆ†å‰²ç½‘ç»œç›¸å…³çš„å¤šç±»äº¤å‰ç†µæŸå¤±å’Œè’¸é¦æŸå¤±ã€‚

* arxiv 2022 paperï¼ˆ[Normalized Feature Distillation for Semantic Segmentation](https://arxiv.org/pdf/2207.05256.pdf)ï¼‰ï¼šè¯¥æ–‡é€šè¿‡å¯¹å…ˆå‰ç‰¹å¾è’¸é¦çš„æ–¹æ³•è¿›è¡Œæ€è€ƒï¼Œè®¤ä¸ºè™½ç„¶å¯¹å­¦ç”Ÿå’Œè€å¸ˆçš„ç‰¹å¾ä¸è¿›è¡Œè½¬æ¢ä¸èƒ½ä½¿å­¦ç”Ÿçš„ç‰¹å¾åˆ†å¸ƒä¸è€å¸ˆç›¸ä¼¼(ä¸‹å›¾çš„bä¸­çš„Navieä»£è¡¨ä¸æ²¡æœ‰è¿›è¡Œè½¬æ¢çš„è’¸é¦ä¸­å­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾åˆ†å¸ƒå’Œè€å¸ˆæ¨¡å‹ç‰¹å¾çš„CKAç›¸ä¼¼æ€§)ï¼Œä½†è®¤ä¸ºæ³¨æ„åŠ›å›¾ï¼Œæ ¼æ‹‰ç±³çŸ©é˜µå’Œæˆå¯¹ç›¸ä¼¼æ€§ç­‰è½¬æ¢æ–¹æ³•éƒ½æ˜¯çŸ¥è¯†æŸå¤±çš„ï¼Œæ‰€ä»¥ä»–ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„å½’ä¸€åŒ–ç‰¹å¾è’¸é¦ï¼Œå…·ä½“çš„ç‰¹å¾æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img107.jpg)
$$L_{nfd} = D(Norm(F_t), Norm(F_s))$$
å…¶ä¸­$Norm$ä»£è¡¨å½’ä¸€åŒ–ï¼Œ$D$ä»£è¡¨$L_2$è·ç¦»ï¼š
$$\hat{F} = \frac{1}{\sigma}(F-u)$$
$u$å’Œ$\sigma$åˆ†åˆ«ä»£è¡¨ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

æœ€åå®ƒçš„æ€»æŸå¤±è¡¨è¾¾å¼å¦‚ä¸‹ï¼š
$$L = L_{gt} + \lambda_1L_{kd} + \lambda_2L_{nfd}$$
å…¶ä¸­$\lambda_1$è®¾ä¸º10å’Œ$\lambda_2$è®¾ä¸º0.7ã€‚

æœ€ååœ¨å®éªŒç»“æœå’Œæ¶ˆèå®éªŒéƒ¨åˆ†ï¼Œè’¸é¦çš„æå‡åˆ°äº†ä¹‹å‰æ²¡æœ‰çš„é«˜åº¦ï¼Œä¹Ÿè¯æ˜äº†åœ¨ï¼ˆWï¼ŒHï¼‰ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–è’¸é¦çš„æ•ˆæœæ˜¯æœ€å¥½çš„ã€‚(SKDå³æ˜¯ä¸Šä¸€ç¯‡æ–‡ç« [Structured Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf))
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img108.jpg)
![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img109.jpg)