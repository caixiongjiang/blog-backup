---
title: "亚信算法实习笔记"
date: 2023-07-21T18:18:05+08:00
lastmod: 2023-07-28T09:19:06+08:00
draft: false
featured_image: "https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/shixi_tiltle.jpg"
description: "在实习期间遇到的一些问题，以及解决思路和方案"
tags:
- 实习笔记
categories:
- 实习
series:
- 实习笔记
comment : true
---


## 亚信图像算法实习笔记

### 授权书区域识别项目：2023.7.24～2023.8.05

#### 修改Linux服务器文件权限问题

* 将文件设置为可读写执行权限：

```shell
$ chmod 777 file
```

* 给文件所有者增加写权限：

```shell
$ chmod u+w file
```

* 给文件所有者和同组用户赋予读写权限，其他用户只有读权限：

```shell
$ chmod 664 file
```

* 递归修改目录及其子目录中的文件权限：

```shell
$ chmod -R 755 directory
```

* 显示修改后的权限信息：

```shell
$ chmod -v 755 file
```

请注意，修改文件或目录的权限需要有足够的权限进行操作。只有文件或目录的所有者或超级用户(root)才能更改权限。



#### Docker配置深度学习环境

> 第一步，安装Docker

* 检查docker是否安装：

```shell
$ docker help
```

* 如果没有安装docker，则使用官方提供的脚本进行安装：

```shell
$ curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
```

> Docker镜像加速

* 在`/etc/docker/daemon.json`中写入如下内容，如果没有该文件则新建：

```scss
{"registry-mirrors":["https://XXX.mirror.aliyuncs.com/"]}
```

* 重启Docker服务：

```shell
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
```

> 从Docker Hub下载镜像

* 进入[Docker Hub](https://hub.docker.com/)，因为我使用的是pytorch的训练框架，搜索`torch1.9.0-cuda11.1-cudnn8`
* 点击左边的`tags`，复制拉取镜像的脚本，在服务器的命令行上运行

> 运行Docker容器

* 下载完容器镜像之后，查看所有images：

```shell
$ docker images
```

* 找到自己的容器，启动该容器：

```shell
$ docker run -it mindest/torch1.9.0-cuda11.1-cudnn8:bevt /bin/bash 
```

参数说明：

`-i`：交互式操作

`-t`：终端

`mindest/torch1.9.0-cuda11.1-cudnn8:bevt`：镜像名称：镜像标签

`bin/bash`：放在镜像后面的是命令，这里我们希望有个交互式 Shell，因此用的是bin/bash

> 在容器内安装所需要的包，并更新镜像

* 安装需要的包，直接使用`pip install`和`conda install`
* 更新镜像：容器是动态的，镜像是静态的。我们在容器里更新了Python包，为了以后可以持久地使用，还需要使用`commit`将容器打包为镜像。

```shell
$ docker commit -m="update packages" -a="XXX" bb8967093b48 XXX/torch1.9.0-cuda11.1-cudnn8:bevt
```

各个参数说明：

- `-m`: 提交的描述信息
- `-a`: 指定镜像作者
- `bb8967093b48`：容器 ID
- `XXX/mypymarl:v1`: 指定要创建的目标镜像名（作者名/镜像名：标签）

> 在本地使用容器运行代码

* 首先我们需要创建一个本地的Ubuntu系统和docker容器共享的文件夹：

```shell
$ sudo mkdir /data
$ sudo docker run -v /data:/data -itd caixj/pytorch:v1
```

* 查看正在运行的容器:

```shell
$ docker ps
```

* **找到我们容器的ID**，并进入该容器

```shell
$ docker attach 500ad76de1cf
```

> 安装nvdia-cuda

Docker 默认是不支持在容器内 GPU 加速的，NVIDIA 官方做了个工具箱来支持容器内 GPU 加速运算，这大大方便了深度学习开发者。这里直接根据官方教程安装即可。

[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

安装完nvidia-cuda之后，再创建容器时加上`--gpus all`，即可在容器内调用cuda，即

```shell
sudo docker run -v /data:/data -itd --gpus all caixj/pytorch:v1 bash
```

然后跟上述步骤相同，进入容器，然后运行代码就ok。

#### Docker常用命令

* 查看所有镜像

```text
docker images
```

* 查找镜像

```text
docker search XXX/image
```

* 下载镜像

```text
docker pull XXX/images:tag
```

* 删除镜像

```text
docker rmi XXX/images:ta
```

* 启动容器

```text
docker run -it image:tag /bin/bash
```

* 退出容器

```text
exi
```

*  查看正在运行的容器

```text
docker ps
```

*  进入正在运行的容器

```text
docker attach container_ID
```

* 查看已停止运行的容器

```text
docker ps -a
```

* 启动已停止的容器

```text
docker start container_ID
```

* 停止容器

```text
docker stop container_ID
```

*  重启已停止容器

```text
docker restart container_I
```

* 退出容器终端（但不停止）

```text
docker exec
```



#### 目标检测检测框原理

> YOLOv4检测头原理

检测头由一个常规的$3\times 3$卷积接上一个$1\times 1$卷积组成。假设输入图像为$416\times 416$，最后得到的特征图的大小为`(B, 75, 26, 26)`，这里的分类数为20。

那么通道数75是如何得到的呢？

75 = 3 * （5 + 分类数） = 3 * （4 + 1 + 20）= 75

在$26\times 26$的特征层中，会预先标定三个先验框，**YOLOv4网络的预测结果只会判定先验框内部是否包含物体和这个物体的种类以及对先验框进行调整，获得一个新的预测框**。

**所以上面的3代表每一个特征图上的三个先验框。4 + 1中的4代表了先验框的调整参数，1的内容代表先验框内部是否包含物体，num_classes个通道分别代表属于该类的概率。**

> 先验框详解与解码

先看先验框的解码代码：

```python
class DecodeBox():
    def __init__(self, anchors, num_classes, input_shape, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]):
        super(DecodeBox, self).__init__()
        self.anchors        = anchors
        self.num_classes    = num_classes
        self.bbox_attrs     = 5 + num_classes
        self.input_shape    = input_shape
        #-----------------------------------------------------------#
        #   13x13的特征层对应的anchor是[81,82],[135,169],[344,319]
        #   26x26的特征层对应的anchor是[10,14],[23,27],[37,58]
        #-----------------------------------------------------------#
        self.anchors_mask   = anchors_mask

    def decode_box(self, inputs):
        outputs = []
        for i, input in enumerate(inputs):
            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 255, 13, 13
            #   batch_size, 255, 26, 26
            #-----------------------------------------------#
            batch_size      = input.size(0)
            input_height    = input.size(2)
            input_width     = input.size(3)

            #-----------------------------------------------#
            #   输入为416x416时
            #   stride_h = stride_w = 32、16、8
            #-----------------------------------------------#
            stride_h = self.input_shape[0] / input_height
            stride_w = self.input_shape[1] / input_width
            #-------------------------------------------------#
            #   此时获得的scaled_anchors大小是相对于特征层的
            #-------------------------------------------------#
            scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]]

            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 3, 13, 13, 85
            #   batch_size, 3, 26, 26, 85
            #-----------------------------------------------#
            prediction = input.view(batch_size, len(self.anchors_mask[i]),
                                    self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()

            #-----------------------------------------------#
            #   先验框的中心位置的调整参数
            #-----------------------------------------------#
            x = torch.sigmoid(prediction[..., 0])  
            y = torch.sigmoid(prediction[..., 1])
            #-----------------------------------------------#
            #   先验框的宽高调整参数
            #-----------------------------------------------#
            w = prediction[..., 2]
            h = prediction[..., 3]
            #-----------------------------------------------#
            #   获得置信度，是否有物体
            #-----------------------------------------------#
            conf        = torch.sigmoid(prediction[..., 4])
            #-----------------------------------------------#
            #   种类置信度
            #-----------------------------------------------#
            pred_cls    = torch.sigmoid(prediction[..., 5:])

            FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
            LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

            #----------------------------------------------------------#
            #   生成网格，先验框中心，网格左上角 
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor)
            grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor)

            #----------------------------------------------------------#
            #   按照网格格式生成先验框的宽高
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
            anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
            anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)
            anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)

            #----------------------------------------------------------#
            #   利用预测结果对先验框进行调整
            #   首先调整先验框的中心，从先验框中心向右下角偏移
            #   再调整先验框的宽高。
            #----------------------------------------------------------#
            pred_boxes          = FloatTensor(prediction[..., :4].shape)
            pred_boxes[..., 0]  = x.data + grid_x
            pred_boxes[..., 1]  = y.data + grid_y
            pred_boxes[..., 2]  = torch.exp(w.data) * anchor_w
            pred_boxes[..., 3]  = torch.exp(h.data) * anchor_h

            #----------------------------------------------------------#
            #   将输出结果归一化成小数的形式
            #----------------------------------------------------------#
            _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor)
            output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale,
                                conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1)
            outputs.append(output.data)
        return outputs
```

下面展示了一个具体图片的先验框调整过程：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img1.jpg)

可以看到原本的三个先验框的中心点是相同的，调整之后，先验框的中心点发生了偏移。

> YOLOv4的预测过程

计算输入图片的宽高 ——> 将图片转化为RGB图片 ——> 给图片增加灰条，实现不失真的resize ——> 归一化转置后，添加上Batch维度 ——> 将图片输入网络进行预测（需要转为tensor）——> 对输出特征层进行解码 ——> 对预测框进行堆叠，进行非极大抑制



其中非极大抑制过程：取出每一种类的粉最大的框，把它和其他的框进行一个交并比的计算，如果该值大于设置的阈值，则保留这个框。

> VOC检测数据集的格式

该格式的主体目录为：

```scss
- 数据集名称
-|- VOC2007
-|-|- ImageSets（文件夹里面放训练集，验证集，测试集，以txt的形式呈现）
-|-|- JPEGImages（文件夹里面放原图）
-|-|- Annotations（文件夹里面放标签的信息，以xml文件形式存在）
```

其中`Annotations`中对应`JPEGImages`里面每张图片对应一个xml文件，放一个xml文件的格式示例：

```xml
<annotation>
    <folder>VOC2007</folder>
    <filename>ed27edc6-cec1-11ed-96cf-58961d2b4192.jpg</filename>
    <size>
        <width>1000</width>
        <height>750</height>
        <depth>3</depth>
    </size>
    <object>
        <name>class1</name>
        <bndbox>
            <xmin>899</xmin>
            <ymin>29</ymin>
            <xmax>923</xmax>
            <ymax>286</ymax>
        </bndbox>
    </object>
    <object>
        <name>class2</name>
        <bndbox>
            <xmin>814</xmin>
            <ymin>69</ymin>
            <xmax>840</xmax>
            <ymax>319</ymax>
        </bndbox>
    </object>
    <object>
        <name>class3</name>
        <bndbox>
            <xmin>745</xmin>
            <ymin>60</ymin>
            <xmax>761</xmax>
            <ymax>195</ymax>
        </bndbox>
    </object>
    <object>
        <name>class4</name>
        <bndbox>
            <xmin>741</xmin>
            <ymin>307</ymin>
            <xmax>759</xmax>
            <ymax>520</ymax>
        </bndbox>
    </object>
    <object>
        <name>class5</name>
        <bndbox>
            <xmin>693</xmin>
            <ymin>61</ymin>
            <xmax>710</xmax>
            <ymax>211</ymax>
        </bndbox>
    </object>
    <object>
        <name>class6</name>
        <bndbox>
            <xmin>511</xmin>
            <ymin>51</ymin>
            <xmax>613</xmax>
            <ymax>579</ymax>
        </bndbox>
    </object>
    <object>
        <name>class7</name>
        <bndbox>
            <xmin>93</xmin>
            <ymin>304</ymin>
            <xmax>114</xmax>
            <ymax>544</ymax>
        </bndbox>
    </object>
</annotation>
```

其中我们需要的信息只有<object>里面的内容，分别包含了类的信息的标签框的坐标位置信息。

> 通过聚类算法对特定数据集进行先验框的调整

先验框的检测和边框预测都是在 YOLOv4 模型中的头部检测模块中进行的，YOLOv4 模型在设计时没有对 Head 进行改进，使用的仍然是 YOLO_Head 检测头。设定的这些先验框尺寸就是经过聚类算法 K-means 聚类而来的，其中的 K的取值，一般是随机确定一个初始点，然后在距离这个点最远的距离为第二个点，以此类推可以确定多个点。假设K的取值为 2，包括了手机和烟头两种目标类型。

这里设定的先验框会根据网格内是否存在目标对先验框进行维持的微调。映射到本文中的YOLOv4 检测算法中，在生成网格后，在每个网格中生成三个不同尺寸的预测框，这三个预测框会对出租车司机违规行为数据中的手机和烟头目标进行微调得到真实框，并通过先验框和真实框求出它们之间的 IOU (交并比)，通过交并比来判断出更真实的检测框。

#### 远程使用服务器上的Tensorboard

本机操作系统：MacOS 12

服务器系统：Ubuntu

> 如何远程使用TensorBoard

* 连接ssh时，将服务器的6006端口重定向到自己的机器上

```shell
$ ssh -L 16006:127.0.0.1:6006 username@remote_server_ip
```

或者

```shell
$ ssh -L 8008:localhost:6006 username@remote_server_ip
```

* 在服务器上使用6006端口正常启动tensorboard：

```shell
$ tensorboard --logdir=xxx --port=6006
```

* 在本地浏览器输入地址：

```scss
127.0.0.1:16006
```

#### 授权书特定区域检测思路

授权书的需求如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img2.jpg)

给定一张移动公司的授权书模版，需要将代表不同区域的位置给分割出来，然后交给文字OCR进行识别。

由于文字OCR已经做好了，给我的需求就是将该图上这些区域给分割出来。

> 难点解析

* 1.该需求只对特定的信息区域进行检测。
* 2.对于特定文字区域来说，对于整张图片来说，语义信息与其他文字区域相似；且在这些区域中，存在手写体和打印体。区域在拖片中的位置是不固定的，也就是说授权书的摆放位置是不固定的（横竖摆放都存在）。
* 对检测的准确率要求较高，也就是Precsion较高，因为检测不准，那么就会漏文字。

> 解决思路

* 1.针对语义信息不足的问题，拟采用分两阶段检测的方法进行。第一阶段检测带文字属性字段和属性信息的联合区域的检测。如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img3.jpg)

第二阶段将除了检测到的区域像素全部置0（加mask）。再对我们的属性信息部分区域进行检测。将难度较高的任务分为两个更简单的任务的组合。

* 2.yolo系列算法集成较好，使用起来成本不会特别高。（众所周知，改进结构的效果是有限的，我们不是在科研，能解决问题就行）。
* 3.对于分类的问题，第一阶段应该进行分类。首先不同的类之间在图片中的位置信息不同，周围的文字分布信息也不同。根据这个点可以比较好分类。如果将所有区域分为1类，则很难找到它们联合的特征与其他文本区域特征的区别。
* 4.对于第二阶段的检测，应该注重准确率，目前想法分为1类。因为它们的特征比较明显，要么有下划线（红框区域中），要么与其他文本区域有明显的界限。

#### YOLO格式标签——>VOC检测格式

YOLO数据集格式：

```scss
# images下面的train和val放图片，labels下面的train和val下面放标签（txt文件格式）
- dataset
-- score
--- images
---- train
---- val
--- labels
---- train
---- val
```

YOLO格式的检测标签较为简单，例子：

```scss
# 从左到右分别代表：
# 类别 检测框的中心x坐标 检测框的中心y坐标 检测框的高 检测框的宽
# 上面提及的坐标都已经归一化，归一化的标准是按照原始影像大小进行归一化
0 0.915000 0.234667 0.034000 0.381333
1 0.813000 0.280667 0.034000 0.329333
2 0.721000 0.238667 0.026000 0.229333
3 0.725000 0.624667 0.034000 0.294667
4 0.661500 0.246667 0.025000 0.234667
5 0.505500 0.498000 0.131000 0.726667
6 0.045000 0.646667 0.040000 0.330667
```

那么如何将该格式转化为VOC格式的检测数据(.xml文件)呢？这涉及部分数学计算。代码如下：

```python
import os
import cv2
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom

def yolo_to_voc(yolo_annotation, image_path, new_label_folder):
    
    os.makedirs(new_label_folder, exist_ok=True)

    
    class_names = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7']  # Replace with your class names

    image = cv2.imread(image_path)
    height, width, _ = image.shape

    root = ET.Element("annotation")

    folder = ET.SubElement(root, "folder")
    folder.text = "VOC2007"

    filename = ET.SubElement(root, "filename")
    filename.text = os.path.basename(image_path)

    size = ET.SubElement(root, "size")
    width_elem = ET.SubElement(size, "width")
    width_elem.text = str(width)
    height_elem = ET.SubElement(size, "height")
    height_elem.text = str(height)
    depth_elem = ET.SubElement(size, "depth")
    depth_elem.text = str(3)  # Assuming 3-channel images

    for line in yolo_annotation:
        class_id, x_center, y_center, w, h = map(float, line.split())

        class_name = class_names[int(class_id)]
        x_min = int((x_center - w / 2) * width)
        y_min = int((y_center - h / 2) * height)
        x_max = int((x_center + w / 2) * width)
        y_max = int((y_center + h / 2) * height)

        object_elem = ET.SubElement(root, "object")
        name_elem = ET.SubElement(object_elem, "name")
        name_elem.text = class_name

        bbox = ET.SubElement(object_elem, "bndbox")
        xmin_elem = ET.SubElement(bbox, "xmin")
        xmin_elem.text = str(x_min)
        ymin_elem = ET.SubElement(bbox, "ymin")
        ymin_elem.text = str(y_min)
        xmax_elem = ET.SubElement(bbox, "xmax")
        xmax_elem.text = str(x_max)
        ymax_elem = ET.SubElement(bbox, "ymax")
        ymax_elem.text = str(y_max)

    xml_path = os.path.join(new_label_folder, os.path.basename(image_path).replace('.jpg', '.xml'))

    # 保存新的XML文件，并使用xml.dom.minidom进行格式化
    xml_string = ET.tostring(root, encoding="utf-8")
    dom = minidom.parseString(xml_string)
    pretty_xml_string = dom.toprettyxml(indent="    ")

    # 去掉XML声明
    pretty_xml_string = '\n'.join([line for line in pretty_xml_string.split('\n') if not line.strip().startswith('<?xml')])

    with open(xml_path, "w", encoding="utf-8") as f:
        f.write(pretty_xml_string)

# Replace with the actual paths to your YOLO annotation files and image folder
yolo_annotation_dir = "授权书标注/label"
image_folder = "授权书标注/biaozhu1"
new_label_folder = "授权书标注/voc_label"

if not os.path.exists(image_folder):
    raise ValueError("Image folder does not exist.")

yolo_annotation_files = os.listdir(yolo_annotation_dir)
for yolo_file in yolo_annotation_files:
    yolo_file_path = os.path.join(yolo_annotation_dir, yolo_file)
    with open(yolo_file_path, 'r') as f:
        yolo_lines = f.readlines()

    image_name = os.path.splitext(yolo_file)[0] + ".jpg"
    image_path = os.path.join(image_folder, image_name)

    if not os.path.exists(image_path):
        raise ValueError(f"Image '{image_name}' not found in the image folder.")

    yolo_to_voc(yolo_lines, image_path, new_label_folder)

```

