---
title: "亚信算法实习笔记"
date: 2023-07-21T18:18:05+08:00
lastmod: 2023-08-01T09:19:06+08:00
draft: false
featured_image: "https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/shixi_tiltle.jpg"
description: "在实习期间遇到的一些问题，以及解决思路和方案"
tags:
- 实习笔记
categories:
- 实习
series:
- 实习笔记
comment : true
---


## 亚信图像算法实习笔记

### 授权书区域识别项目：2023.7.24～2023.8.05

#### 修改Linux服务器文件权限问题

* 将文件设置为可读写执行权限：

```shell
$ chmod 777 file
```

* 给文件所有者增加写权限：

```shell
$ chmod u+w file
```

* 给文件所有者和同组用户赋予读写权限，其他用户只有读权限：

```shell
$ chmod 664 file
```

* 递归修改目录及其子目录中的文件权限：

```shell
$ chmod -R 755 directory
```

* 显示修改后的权限信息：

```shell
$ chmod -v 755 file
```

请注意，修改文件或目录的权限需要有足够的权限进行操作。只有文件或目录的所有者或超级用户(root)才能更改权限。



#### Docker配置深度学习环境

> 第一步，安装Docker

* 检查docker是否安装：

```shell
$ docker help
```

* 如果没有安装docker，则使用官方提供的脚本进行安装：

```shell
$ curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
```

> Docker镜像加速

* 在`/etc/docker/daemon.json`中写入如下内容，如果没有该文件则新建：

```scss
{"registry-mirrors":["https://XXX.mirror.aliyuncs.com/"]}
```

* 重启Docker服务：

```shell
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
```

> 从Docker Hub下载镜像

* 进入[Docker Hub](https://hub.docker.com/)，因为我使用的是pytorch的训练框架，搜索`torch1.9.0-cuda11.1-cudnn8`
* 点击左边的`tags`，复制拉取镜像的脚本，在服务器的命令行上运行

> 运行Docker容器

* 下载完容器镜像之后，查看所有images：

```shell
$ docker images
```

* 找到自己的容器，启动该容器：

```shell
$ docker run -it mindest/torch1.9.0-cuda11.1-cudnn8:bevt /bin/bash 
```

参数说明：

`-i`：交互式操作

`-t`：终端

`mindest/torch1.9.0-cuda11.1-cudnn8:bevt`：镜像名称：镜像标签

`bin/bash`：放在镜像后面的是命令，这里我们希望有个交互式 Shell，因此用的是bin/bash

> 在容器内安装所需要的包，并更新镜像

* 安装需要的包，直接使用`pip install`和`conda install`
* 更新镜像：容器是动态的，镜像是静态的。我们在容器里更新了Python包，为了以后可以持久地使用，还需要使用`commit`将容器打包为镜像。

```shell
$ docker commit -m="update packages" -a="XXX" bb8967093b48 XXX/torch1.9.0-cuda11.1-cudnn8:bevt
```

各个参数说明：

- `-m`: 提交的描述信息
- `-a`: 指定镜像作者
- `bb8967093b48`：容器 ID
- `XXX/mypymarl:v1`: 指定要创建的目标镜像名（作者名/镜像名：标签）

> 在本地使用容器运行代码

* 首先我们需要创建一个本地的Ubuntu系统和docker容器共享的文件夹：

```shell
$ sudo mkdir /data
$ sudo docker run -v /data:/data -itd caixj/pytorch:v1
```

* 查看正在运行的容器:

```shell
$ docker ps
```

* **找到我们容器的ID**，并进入该容器

```shell
$ docker attach 500ad76de1cf
```

> 安装nvdia-cuda

Docker 默认是不支持在容器内 GPU 加速的，NVIDIA 官方做了个工具箱来支持容器内 GPU 加速运算，这大大方便了深度学习开发者。这里直接根据官方教程安装即可。

[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

安装完nvidia-cuda之后，再创建容器时加上`--gpus all`，即可在容器内调用cuda，即

```shell
$ sudo docker run -v /data:/data -itd --gpus all caixj/pytorch:v1 bash
```

然后跟上述步骤相同，进入容器，然后运行代码就ok。

* 本地保存镜像

```shell
$ docker save -o <your_file_name.tar> <image id>
```

* 通过本地镜像导入docker容器

```shell
$ docker load < your_file.tar.gz
# 或者
$ docker load --input your_file.tar
# 它的images的名字会变为your_file:latest
```

#### Docker常用命令

* 查看所有镜像

```text
docker images
```

* 查找镜像

```text
docker search XXX/image
```

* 下载镜像

```text
docker pull XXX/images:tag
```

* 删除镜像

```text
docker rmi XXX/images:ta
```

* 启动容器

```text
docker run -it image:tag /bin/bash
```

* 退出容器

```text
exi
```

*  查看正在运行的容器

```text
docker ps
```

*  进入正在运行的容器

```text
docker attach container_ID
```

* 查看已停止运行的容器

```text
docker ps -a
```

* 启动已停止的容器

```text
docker start container_ID
```

* 停止容器

```text
docker stop container_ID
```

*  重启已停止容器

```text
docker restart container_I
```

* 退出容器终端（但不停止）

```text
docker exec
```



#### 目标检测检测框原理

> YOLOv4检测头原理

检测头由一个常规的$3\times 3$卷积接上一个$1\times 1$卷积组成。假设输入图像为$416\times 416$，最后得到的特征图的大小为`(B, 75, 26, 26)`，这里的分类数为20。

那么通道数75是如何得到的呢？

75 = 3 * （5 + 分类数） = 3 * （4 + 1 + 20）= 75

在$26\times 26$的特征层中，会预先标定三个先验框，**YOLOv4网络的预测结果只会判定先验框内部是否包含物体和这个物体的种类以及对先验框进行调整，获得一个新的预测框**。

**所以上面的3代表每一个特征图上的三个先验框。4 + 1中的4代表了先验框的调整参数，1的内容代表先验框内部是否包含物体，num_classes个通道分别代表属于该类的概率。**

> 先验框详解与解码

先看先验框的解码代码：

```python
class DecodeBox():
    def __init__(self, anchors, num_classes, input_shape, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]):
        super(DecodeBox, self).__init__()
        self.anchors        = anchors
        self.num_classes    = num_classes
        self.bbox_attrs     = 5 + num_classes
        self.input_shape    = input_shape
        #-----------------------------------------------------------#
        #   13x13的特征层对应的anchor是[81,82],[135,169],[344,319]
        #   26x26的特征层对应的anchor是[10,14],[23,27],[37,58]
        #-----------------------------------------------------------#
        self.anchors_mask   = anchors_mask

    def decode_box(self, inputs):
        outputs = []
        for i, input in enumerate(inputs):
            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 255, 13, 13
            #   batch_size, 255, 26, 26
            #-----------------------------------------------#
            batch_size      = input.size(0)
            input_height    = input.size(2)
            input_width     = input.size(3)

            #-----------------------------------------------#
            #   输入为416x416时
            #   stride_h = stride_w = 32、16、8
            #-----------------------------------------------#
            stride_h = self.input_shape[0] / input_height
            stride_w = self.input_shape[1] / input_width
            #-------------------------------------------------#
            #   此时获得的scaled_anchors大小是相对于特征层的
            #-------------------------------------------------#
            scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]]

            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 3, 13, 13, 85
            #   batch_size, 3, 26, 26, 85
            #-----------------------------------------------#
            prediction = input.view(batch_size, len(self.anchors_mask[i]),
                                    self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()

            #-----------------------------------------------#
            #   先验框的中心位置的调整参数
            #-----------------------------------------------#
            x = torch.sigmoid(prediction[..., 0])  
            y = torch.sigmoid(prediction[..., 1])
            #-----------------------------------------------#
            #   先验框的宽高调整参数
            #-----------------------------------------------#
            w = prediction[..., 2]
            h = prediction[..., 3]
            #-----------------------------------------------#
            #   获得置信度，是否有物体
            #-----------------------------------------------#
            conf        = torch.sigmoid(prediction[..., 4])
            #-----------------------------------------------#
            #   种类置信度
            #-----------------------------------------------#
            pred_cls    = torch.sigmoid(prediction[..., 5:])

            FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
            LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

            #----------------------------------------------------------#
            #   生成网格，先验框中心，网格左上角 
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor)
            grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor)

            #----------------------------------------------------------#
            #   按照网格格式生成先验框的宽高
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
            anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
            anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)
            anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)

            #----------------------------------------------------------#
            #   利用预测结果对先验框进行调整
            #   首先调整先验框的中心，从先验框中心向右下角偏移
            #   再调整先验框的宽高。
            #----------------------------------------------------------#
            pred_boxes          = FloatTensor(prediction[..., :4].shape)
            pred_boxes[..., 0]  = x.data + grid_x
            pred_boxes[..., 1]  = y.data + grid_y
            pred_boxes[..., 2]  = torch.exp(w.data) * anchor_w
            pred_boxes[..., 3]  = torch.exp(h.data) * anchor_h

            #----------------------------------------------------------#
            #   将输出结果归一化成小数的形式
            #----------------------------------------------------------#
            _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor)
            output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale,
                                conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1)
            outputs.append(output.data)
        return outputs
```

下面展示了一个具体图片的先验框调整过程：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img1.jpg)

可以看到原本的三个先验框的中心点是相同的，调整之后，先验框的中心点发生了偏移。

> YOLOv4的预测过程

计算输入图片的宽高 ——> 将图片转化为RGB图片 ——> 给图片增加灰条，实现不失真的resize ——> 归一化转置后，添加上Batch维度 ——> 将图片输入网络进行预测（需要转为tensor）——> 对输出特征层进行解码 ——> 对预测框进行堆叠，进行非极大抑制



其中非极大抑制过程：取出每一种类的粉最大的框，把它和其他的框进行一个交并比的计算，如果该值大于设置的阈值，则保留这个框。

> VOC检测数据集的格式

该格式的主体目录为：

```scss
- 数据集名称
-|- VOC2007
-|-|- ImageSets（文件夹里面放训练集，验证集，测试集，以txt的形式呈现）
-|-|- JPEGImages（文件夹里面放原图）
-|-|- Annotations（文件夹里面放标签的信息，以xml文件形式存在）
```

其中`Annotations`中对应`JPEGImages`里面每张图片对应一个xml文件，放一个xml文件的格式示例：

```xml
<annotation>
    <folder>VOC2007</folder>
    <filename>ed27edc6-cec1-11ed-96cf-58961d2b4192.jpg</filename>
    <size>
        <width>1000</width>
        <height>750</height>
        <depth>3</depth>
    </size>
    <object>
        <name>class1</name>
        <bndbox>
            <xmin>899</xmin>
            <ymin>29</ymin>
            <xmax>923</xmax>
            <ymax>286</ymax>
        </bndbox>
    </object>
    <object>
        <name>class2</name>
        <bndbox>
            <xmin>814</xmin>
            <ymin>69</ymin>
            <xmax>840</xmax>
            <ymax>319</ymax>
        </bndbox>
    </object>
    <object>
        <name>class3</name>
        <bndbox>
            <xmin>745</xmin>
            <ymin>60</ymin>
            <xmax>761</xmax>
            <ymax>195</ymax>
        </bndbox>
    </object>
    <object>
        <name>class4</name>
        <bndbox>
            <xmin>741</xmin>
            <ymin>307</ymin>
            <xmax>759</xmax>
            <ymax>520</ymax>
        </bndbox>
    </object>
    <object>
        <name>class5</name>
        <bndbox>
            <xmin>693</xmin>
            <ymin>61</ymin>
            <xmax>710</xmax>
            <ymax>211</ymax>
        </bndbox>
    </object>
    <object>
        <name>class6</name>
        <bndbox>
            <xmin>511</xmin>
            <ymin>51</ymin>
            <xmax>613</xmax>
            <ymax>579</ymax>
        </bndbox>
    </object>
    <object>
        <name>class7</name>
        <bndbox>
            <xmin>93</xmin>
            <ymin>304</ymin>
            <xmax>114</xmax>
            <ymax>544</ymax>
        </bndbox>
    </object>
</annotation>
```

其中我们需要的信息只有<object>里面的内容，分别包含了类的信息的标签框的坐标位置信息。

> 通过聚类算法对特定数据集进行先验框的调整

先验框的检测和边框预测都是在 YOLOv4 模型中的头部检测模块中进行的，YOLOv4 模型在设计时没有对 Head 进行改进，使用的仍然是 YOLO_Head 检测头。设定的这些先验框尺寸就是经过聚类算法 K-means 聚类而来的，其中的 K的取值，一般是随机确定一个初始点，然后在距离这个点最远的距离为第二个点，以此类推可以确定多个点。假设K的取值为 2，包括了手机和烟头两种目标类型。

这里设定的先验框会根据网格内是否存在目标对先验框进行维持的微调。映射到本文中的YOLOv4 检测算法中，在生成网格后，在每个网格中生成三个不同尺寸的预测框，这三个预测框会对出租车司机违规行为数据中的手机和烟头目标进行微调得到真实框，并通过先验框和真实框求出它们之间的 IOU (交并比)，通过交并比来判断出更真实的检测框。

#### 远程使用服务器上的Tensorboard

本机操作系统：MacOS 12

服务器系统：Ubuntu

> 如何远程使用TensorBoard

* 连接ssh时，将服务器的6006端口重定向到自己的机器上

```shell
$ ssh -L 16006:127.0.0.1:6006 username@remote_server_ip
```

或者

```shell
$ ssh -L 8008:localhost:6006 username@remote_server_ip
```

* 在服务器上使用6006端口正常启动tensorboard：

```shell
$ tensorboard --logdir=xxx --port=6006
```

* 在本地浏览器输入地址：

```scss
127.0.0.1:16006
```

#### 授权书特定区域检测思路

授权书的需求如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img2.jpg)

给定一张移动公司的授权书模版，需要将代表不同区域的位置给分割出来，然后交给文字OCR进行识别。

由于文字OCR已经做好了，给我的需求就是将该图上这些区域给分割出来。

> 难点解析

* 1.该需求只对特定的信息区域进行检测。
* 2.对于特定文字区域来说，对于整张图片来说，语义信息与其他文字区域相似；且在这些区域中，存在手写体和打印体。区域在拖片中的位置是不固定的，也就是说授权书的摆放位置是不固定的（横竖摆放都存在）。
* 3.对检测的准确率要求较高，也就是Precsion较高，因为检测不准，那么就会漏文字。

> 解决思路

* 1.针对语义信息不足的问题，拟采用分两阶段检测的方法进行。第一阶段检测带文字属性字段和属性信息的联合区域的检测。如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/%E5%AE%9E%E4%B9%A0/img3.jpg)

第二阶段将除了检测到的区域像素全部置0（加mask）。再对我们的属性信息部分区域进行检测。将难度较高的任务分为两个更简单的任务的组合。

* 2.yolo系列算法集成较好，使用起来成本不会特别高。（众所周知，改进结构的效果是有限的，我们不是在科研，能解决问题就行）。
* 3.对于分类的问题，第一阶段应该进行分类。首先不同的类之间在图片中的位置信息不同，周围的文字分布信息也不同。根据这个点可以比较好分类。如果将所有区域分为1类，则很难找到它们联合的特征与其他文本区域特征的区别。
* 4.对于第二阶段的检测，应该注重准确率，目前想法分为1类。因为它们的特征比较明显，要么有下划线（红框区域中），要么与其他文本区域有明显的界限。

#### YOLO格式标签——>VOC检测格式

YOLO数据集格式：

```scss
# images下面的train和val放图片，labels下面的train和val下面放标签（txt文件格式）
- dataset
-- score
--- images
---- train
---- val
--- labels
---- train
---- val
```

YOLO格式的检测标签较为简单，例子：

```scss
# 从左到右分别代表：
# 类别 检测框的中心x坐标 检测框的中心y坐标 检测框的高 检测框的宽
# 上面提及的坐标都已经归一化，归一化的标准是按照原始影像大小进行归一化
0 0.915000 0.234667 0.034000 0.381333
1 0.813000 0.280667 0.034000 0.329333
2 0.721000 0.238667 0.026000 0.229333
3 0.725000 0.624667 0.034000 0.294667
4 0.661500 0.246667 0.025000 0.234667
5 0.505500 0.498000 0.131000 0.726667
6 0.045000 0.646667 0.040000 0.330667
```

那么如何将该格式转化为VOC格式的检测数据(.xml文件)呢？这涉及部分数学计算。代码如下：

```python
import os
import cv2
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom

def yolo_to_voc(yolo_annotation, image_path, new_label_folder):
    
    os.makedirs(new_label_folder, exist_ok=True)

    
    class_names = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7']  # Replace with your class names

    image = cv2.imread(image_path)
    height, width, _ = image.shape

    root = ET.Element("annotation")

    folder = ET.SubElement(root, "folder")
    folder.text = "VOC2007"

    filename = ET.SubElement(root, "filename")
    filename.text = os.path.basename(image_path)

    size = ET.SubElement(root, "size")
    width_elem = ET.SubElement(size, "width")
    width_elem.text = str(width)
    height_elem = ET.SubElement(size, "height")
    height_elem.text = str(height)
    depth_elem = ET.SubElement(size, "depth")
    depth_elem.text = str(3)  # Assuming 3-channel images

    for line in yolo_annotation:
        class_id, x_center, y_center, w, h = map(float, line.split())

        class_name = class_names[int(class_id)]
        x_min = int((x_center - w / 2) * width)
        y_min = int((y_center - h / 2) * height)
        x_max = int((x_center + w / 2) * width)
        y_max = int((y_center + h / 2) * height)

        object_elem = ET.SubElement(root, "object")
        name_elem = ET.SubElement(object_elem, "name")
        name_elem.text = class_name

        bbox = ET.SubElement(object_elem, "bndbox")
        xmin_elem = ET.SubElement(bbox, "xmin")
        xmin_elem.text = str(x_min)
        ymin_elem = ET.SubElement(bbox, "ymin")
        ymin_elem.text = str(y_min)
        xmax_elem = ET.SubElement(bbox, "xmax")
        xmax_elem.text = str(x_max)
        ymax_elem = ET.SubElement(bbox, "ymax")
        ymax_elem.text = str(y_max)

    xml_path = os.path.join(new_label_folder, os.path.basename(image_path).replace('.jpg', '.xml'))

    # 保存新的XML文件，并使用xml.dom.minidom进行格式化
    xml_string = ET.tostring(root, encoding="utf-8")
    dom = minidom.parseString(xml_string)
    pretty_xml_string = dom.toprettyxml(indent="    ")

    # 去掉XML声明
    pretty_xml_string = '\n'.join([line for line in pretty_xml_string.split('\n') if not line.strip().startswith('<?xml')])

    with open(xml_path, "w", encoding="utf-8") as f:
        f.write(pretty_xml_string)

# Replace with the actual paths to your YOLO annotation files and image folder
yolo_annotation_dir = "授权书标注/label"
image_folder = "授权书标注/biaozhu1"
new_label_folder = "授权书标注/voc_label"

if not os.path.exists(image_folder):
    raise ValueError("Image folder does not exist.")

yolo_annotation_files = os.listdir(yolo_annotation_dir)
for yolo_file in yolo_annotation_files:
    yolo_file_path = os.path.join(yolo_annotation_dir, yolo_file)
    with open(yolo_file_path, 'r') as f:
        yolo_lines = f.readlines()

    image_name = os.path.splitext(yolo_file)[0] + ".jpg"
    image_path = os.path.join(image_folder, image_name)

    if not os.path.exists(image_path):
        raise ValueError(f"Image '{image_name}' not found in the image folder.")

    yolo_to_voc(yolo_lines, image_path, new_label_folder)

```

#### YOLOv8专用框架

YOLOv8官方的代码库非常完善，官方提供了命令行（CLI）和Python接口，封装地相当好。

[YOLOv8官方代码库](https://github.com/ultralytics/ultralytics)

[YOLOv8官方文档](https://docs.ultralytics.com/)

> 环境准备

环境准备其实非常简单，准备一个能使用CUDA和cuDNN的环境就好。由于我们使用的是其封装好的接口，不是优化源代码，所以我们只需要下载一个`ultralytics`的包就ok了：

```shell
$ pip install ultralytics
```

如果需要使用docker，它也在`Docker-Hub`上集成好了镜像：[https://hub.docker.com/r/ultralytics/ultralytics](https://hub.docker.com/r/ultralytics/ultralytics)，也支持`Gradient`，`Colab`，`Kaggle`平台使用，对白嫖党也很友好。

> 数据准备

YOLOv8使用的是yolo格式的数据集。因为我使用的是其检测的分支，所以我们准备一个检测的数据集。数据集的文件组织方式如下：

```scss
- 数据集名称
-- dataset
--- score
---- images
----- train
...//放置图片
----- val
...//放置图片
---- labels
----- train
...//放置txt标签文件
----- val
...//放置txt标签文件
```

编写一个关于数据集的yaml文件，`data.yaml`:

```yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
train: ./YOLO_Authorize_letter/dataset/score/images/train # train images
val: ./YOLO_Authorize_letter/dataset/score/images/val # val images
# test:  # test images (optional)

# Classes 注意这里的类别和标签的类别要对应
names:
  0: class1
  1: class2
  2: class3
  3: class4
  4: class5
  5: class6
  6: class7
```

使用官方提供的模型yaml文件，`yolov8.yaml`：

```yaml
# Ultralytics YOLO 🚀, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect

# Parameters
nc: 7  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs

# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9

# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)

```

使用该文件需要注意的是：`nc`需要根据自己数据集的分类数进行改变，与上面的`data.yaml`的类别需要对应。

> 训练准备

新建一个YOLOv8根目录，将数据集以及上述两个文件放置在根目录上，运行官方给的训练示例：

```shell
$ yolo train data=coco128.yaml model=yolov8m.yaml epochs=10 lr0=0.01
```

需要注意的是虽然我们的文件是`yolov8.yaml`，我们使用`yolov8n.yaml`会直接进行解析，非常方便。

但简单的参数调整不足以使用，我们要获取全局的参数调整，可以使用下面这种方式：

* 首先在命令行输入：

```shell
$ yolo copy-cfg
```

这时可以看到根目录出现了一个`default_copy.yaml`文件

* 修改`default_copy.yaml`文件的内容并保存，每个参数的含义可以看官方文档，这里给几个训练时重要的参数：
  * task: 提供的选项有*detect, segment, classify, pose*，分别对应检测，分割，分类，姿态估计。
  * mode: 提供的选项有*train, val, predict, export, track, benchmark*，分别代表训练，验证，预测，导出，跟踪，基准测试。填入`train`。
  * model: 可以使用pt文件，也可以使用yaml文件。这里我们使用`yolov8m.yaml`
  * data: 指定的数据集配置文件，这里我们使用`data.yaml`
  * epoch: 训练的轮数，**数据集越小，需要的轮数越多**。
  * patience: 指定有多少轮后指标名优明显的增长则停止训练（early stop的训练策略）。使用默认的50
  * batch: 根据GPU的显存来合理设置，一般为2的次方
  * imgsz: 一般使用640
  * device: 指定GPU设备，有三种选项，单卡，多卡和cpu，分别对应*device=0 or device=0,1,2,3 or device=cpu*。在Linux上可以使用`nvidia-smi`来查看你的GPU设备信息。
  * workers: 代表多线程读取数据，建议指定低一点，尤其是服务器上多人使用时，指定过多就会报错。我们制定为2。

* 覆盖默认的配置进行训练：

```shell
$ yolo yolo cfg='default_copy.yaml'
```

* 查看训练的结果：

训练结果会在根目录下的`runs/detect/train`文件夹下，如果是第二次启动训练则会在`runs/detect/train2`文件夹下。也可以通过终端提示的命令在TensorBoard查看，如果是服务器上，就需要使用服务器端口映射的服务，在上文有讲。

> 用训练得到的模型进行预测

我们仍然可以使用CLI的方式运行，只需要修改mode的model以及predict那一栏的参数。但我并不推荐这种方式，因为我们通常的目的并不是单纯检测，而是为了检测出后获取坐标进行后续操作。

所以我们使用python接口的方式，官方给的小示例：

```python
from ultralytics import YOLO

model = YOLO("runs/detect/train/weights/best.pt")
# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam
# source这里放存放需要预测图片的文件位置
results = model.predict(source="...", save=True) 
```

**需要注意的是predict中接受的参数还有很多，比如比较重要的有`conf`，代表我们保留预测框的置信度，`imgsz`代表模型推理的输入尺寸大小，默认为（640，480），`line_width`代表检测框的粗心，只能为int值。更多的参数设置参考官方文档**

预测的结果会保存在`runs/detect/predict`，如果是第二次预测会保存在`runs/detect/predict2`，依次类推。

为了了解results的内容，将一张图片的预测的结果results输出一下：

```scss
boxes: ultralytics.engine.results.Boxes object
keypoints: None
keys: ['boxes']
masks: None
names: {0: 'company_name', 1: 'signature', 2: 'authorized_name', 3: 'telephone_number', 4: 'id_number', 5: 'mobile_branch_name', 6: 'date'}
orig_img: array([[[ 94, 107, 123],
        [ 93, 106, 122],
        [ 92, 105, 121],
        ...,
        [ 81, 102, 117],
        [ 79, 100, 115],
        [ 78,  99, 114]],

       [[ 95, 108, 124],
        [ 94, 107, 123],
        [ 93, 106, 122],
        ...,
        [ 77,  98, 113],
        [ 77,  98, 113],
        [ 78,  99, 114]],

       [[ 95, 108, 124],
        [ 95, 108, 124],
        [ 94, 107, 123],
        ...,
        [ 76,  97, 112],
        [ 76,  97, 112],
        [ 78,  99, 114]],

       ...,

       [[100, 120, 138],
        [100, 120, 138],
        [ 99, 119, 137],
        ...,
        [115, 135, 153],
        [113, 133, 151],
        [114, 134, 152]],

       [[101, 118, 137],
        [101, 118, 137],
        [101, 118, 137],
        ...,
        [116, 136, 154],
        [115, 135, 153],
        [114, 134, 152]],

       [[100, 117, 136],
        [100, 117, 136],
        [100, 117, 136],
        ...,
        [118, 138, 156],
        [117, 137, 155],
        [113, 133, 151]]], dtype=uint8)
orig_shape: (3648, 2736)
path: '/data/caixj/ultralytics-main/test_imgs/ed257e76-cec1-11ed-b87c-58961d2b4192.jpg'
probs: None
save_dir: None
speed: {'preprocess': 3.3495426177978516, 'inference': 32.27806091308594, 'postprocess': 1.6703605651855469}]
```

结合官方文档，发现我们需要的检测框的坐标信息在boxes对象中。boxes对象的属性信息如下：

```
boxes.xyxy  # 检测框的绝对值坐标，分别是min_x,min_y,max_x,max_y
boxes.xywh  # 检测框的绝对值坐标，分别是中心点坐标和宽高
boxes.xyxyn  # 检测框的归一化坐标，分别是min_x,min_y,max_x,max_y 
boxes.xywhn  # 检测框的归一化坐标，分别是中心点坐标和宽高
boxes.conf  # 检测框的置信分数
boxes.cls  # 类别, (N, )
boxes.data  # 原始目标框参数坐标 (x, y, w, h)、置信度以及类别, (N, 6) or boxes.boxes
```

为了完成我们切割区域的需求，我们来编写一个小的脚本进行批量操作：

```python
from ultralytics import YOLO
from PIL import Image
import cv2
import numpy as np
import os


def process_image_with_yolov8(image, detections, image_out):
    
    # 获取原始图像的尺寸
    height, width = image.shape[:2]

    # 创建全黑遮罩，与原始图像尺寸相同
    mask = np.zeros((height, width), dtype=np.uint8)

    # 将检测到的区域设为非零值
    for box in detections:
        x_min, y_min, x_max, y_max = box[:4]
        x_min, x_max = int(x_min * width), int(x_max * width)
        y_min, y_max = int(y_min * height), int(y_max * height)
        mask[y_min:y_max, x_min:x_max] = 255  # 设置为255，即白色，也可以设置为1
        
    
    # 将原始图像与遮罩图像相乘，得到只有检测区域保持原始像素值，其他区域都变为0的图像
    result_image = cv2.bitwise_and(image, image, mask=mask)

    # 保存结果图
    cv2.imwrite(image_out, result_image)

if __name__ == "__main__":
    image_dir = "test_imgs"
    images_process = "images_process"

    model = YOLO("runs/detect/train/weights/best.pt")
    # accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam
    results = model.predict(source=image_dir, save=False, conf=0.45, device=1, line_width=2) # 
    # print(results)

    for i in range(len(results)):
      	# 读取results结果内的原始图像
        # 最好不要通过cv2和PIL等重新读入，实测和预测框的结果会有偏差
        image = results[i].orig_img
        image_name = results[i].path.split('/')[-1]
        image_out = os.path.join(images_process, image_name)
        result = results[i].cpu().numpy()
        # 获取归一化的坐标
        detections = result.boxes.xyxyn
        print("该张图像的检测框坐标分别为：")
        print(detections)
        process_image_with_yolov8(image, detections, image_out) 
```

#### Log日志使用

> log日志的级别

`logging` 模块定义了几个不同级别的日志，从高到低的顺序如下：

1. `CRITICAL`（最高级别）：用于指示严重的错误，可能导致程序无法继续运行。
2. `ERROR`：用于指示程序运行时的错误，但程序仍然可以继续运行。
3. `WARNING`：用于表示警告信息，可能表明程序出现了一些意外情况，但仍然在正常运行。
4. `INFO`：用于一般性的信息记录，用来追踪程序的执行状态。
5. `DEBUG`（最低级别）：用于调试目的，记录详细的调试信息，一般在开发和调试阶段使用。

这些日志级别的区别在于它们的严重程度和目的。你可以根据不同的场景和需求，选择适当的日志级别来记录信息。

如果你希望同时保存多个级别的日志，可以使用多个日志处理程序，每个处理程序负责处理一个特定级别的日志。以下是一个示例，展示了如何同时保存 `DEBUG`、`INFO`、`WARNING`、`ERROR` 和 `CRITICAL` 五个级别的日志。

```python
import os
import logging
from datetime import datetime

# 确保logs文件夹存在
if not os.path.exists('logs'):
    os.makedirs('logs')

# 获取当前时间并格式化为字符串
current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')

# 设置日志文件名
log_filename = os.path.join('logs', f'{current_time}-server.log')

# 配置日志
logging.basicConfig(filename=log_filename, level=logging.DEBUG,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# 获取一个Logger实例
logger = logging.getLogger(__name__)

# 创建一个文件处理程序来保存所有级别的日志
file_handler = logging.FileHandler(log_filename)
file_handler.setLevel(logging.DEBUG)

# 创建一个控制台处理程序来输出INFO级别以上的日志到控制台
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# 创建一个格式化器
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# 将处理程序添加到Logger实例
logger.addHandler(file_handler)
logger.addHandler(console_handler)

def process_data(data):
    try:
        # ... 一些处理逻辑
        
        # 记录日志
        logger.debug("Debug message: %s", data)
        logger.info("Info message: %s", data)
        logger.warning("Warning message: %s", data)
        logger.error("Error message: %s", data)
        logger.critical("Critical message: %s", data)
        
        return "Data processed successfully."
    except Exception as e:
        # 记录异常日志
        logger.exception("An exception occurred: %s", str(e))
        return "An error occurred."

if __name__ == '__main__':
    data = "Some data to process"
    result = process_data(data)
    print(result)
```

在上述示例中，我们创建了一个文件处理程序 `file_handler` 来保存所有级别的日志，并将它添加到 Logger 实例中。同时，我们创建了一个控制台处理程序 `console_handler` 来输出 `INFO` 级别以上的日志到控制台，并将它也添加到 Logger 实例中。这样，你就可以同时保存 `DEBUG`、`INFO`、`WARNING`、`ERROR` 和 `CRITICAL` 五个级别的日志，并且在控制台上输出一部分级别的信息。

#### 接口的封装

该部分由于涉及公司的代码，只讲思路。通常打包需要两个文件，一个是包含环境变量的镜像文件，另一个则是项目的源代码包。

工程的标准：

* 一般需要将可以设置的参数文件统一集成到一个配置文件中。如果配置参数过多，还可以分文件存储，一般放在`config`文件夹下。
* 模型参数权重则一般放在一个专门的`models`或者`checkpoints`文件夹下面
* 使用onnx，trnsorrt推理，是不需要模型文件的。如果直接用pytorch推理，一般需要一个模型文件，以及一些前处理和后处理的文件。这里使用的是yolov8官方提供的推理包，则不需要模型文件了。
* 主函数不要集成太多的方法，尽量分文件写
* 程序中需要包含详细的日志记录，并需要自动保存在一个单独的文件夹中。一般使用`logs`文件夹。